my name is I studied at the University of A in Colombia and graduated in 2012 with a degree in electronic engineering. Since graduating, I have gained experience in various IT roles, including software development, database administration, and DevOps.

My name is Sergio Hurtado. I'm an electronic engineer and I've been working in the IT area for about 9 years, in these years, I've gained experience in various IT roles, including software development, database administration, and DevOps.

I have about 5 years of experience in DevOps role, and I am certified as an Associate Architect in AWS and Azure Fundamentals. I have knowledge of tools like Git, 
Jenkins, Azure DevOps, github action, Terraform, Ansible, Docker, Kubernetes, Prometheus, Grafana, and script development in Python and Bash.

I am currently working with kubernetes on AKS clusters validate the operation of pipelines in harness and create some alerts in grafana 

2 jenkins
3-4 aws EC2, S·, Lambda, ROUTE53, SNS, SQS, EKS, RDS,IAM,APIGATEWAY,ECR, VPN
2 azure
3 terraform
2 ansible
3 python
3 docker
2 kubernetes
3 grafana
2  prometheus

databrick 10 meses
1 datadog
1 argocd
__________

# ROUTINE

- Review the current infrastructure status on AWS, Kubernetes clusters, and other cloud platforms. Check monitoring dashboards (e.g., Grafana, Prometheus) for any anomalies or performance issues.

- Work on infrastructure improvements and optimizations. Update Terraform configurations to automate provisioning and ensure scalability of resources. Investigate and resolve any infrastructure-related tickets raised by the team.

- Dive into CI/CD pipeline configurations. Review Jenkins jobs and pipelines to ensure smooth integration and deployment processes. Troubleshoot any failed builds or deployment issues and implement necessary fixes.
__________

___________________
# LEAVE JOB
I want to leave my current job because i feel run out of room to grow and I think i need new challenges
__________________

what caught my eye was your focus on remote work, 

the wide range of clients you have in diferrent buisness areas, and the possibility of growing personally and professionally

I know your company has a variety of clients, you guarantee long-term assignments, give certification opportunities, and find projects that match employee knowledge.

______________
# future carrer
I would like to continue polishing my knowledge in cloud, continue with the study of artificial intelligence and improve in aws and obtain new certifications.

_________________

I know your company has a variety of clients, you guarantee long-term assignments, give certification opportunities, and find projects that match employee knowledge.

_____________________
# attributes
I'm fast learner I think its important in this industry because time to time new tools appear
 always seeking out new opportunities to expand my knowledge and skills. Adaptable and persistent

# improvement weakness point

One area I've been actively working on is time management. While I am generally very organized, I sometimes find myself taking on too many tasks at once, which can lead to moments of feeling overwhelmed. However, I've been implementing strategies such as prioritizing tasks and using time-blocking techniques to improve in this area
_______________

I need two weeks to resign-

___________
## Challenge situation on work 

A coworker resigned, leaving behind a project responsible for provisioning test data to QA teams. The challenge was twofold: the lack of clear documentation for the Java application he developed and the looming deadline. To ensure timely delivery, we organized tasks and dove into understanding the code. Additionally, we had to deploy the application on AWS using CloudFormation and implement Azure DevOps for CI/CD. Creating necessary tests to meet SonarQube thresholds added to the complexity. Despite the significant time investment, we successfully completed the task. Our dedication was recognized by the director, motivating us to further improve project performance.

## Achiviement

My greatest achievement was when I was assigned to a project that wasn't receiving the correct reception from clients. We then gathered as a team and analyzed what changes we could make to improve our performance for our clients. During this brainstorming session, an idea emerged that we developed and presented to the project manager. After receiving approval, we deployed it into production, and it was a success. As a result, we received recognition from the company.

## Dificult

"Well, there was a time at work when two different high-level executives asked me to do opposite things. This put me in a very awkward position because I didn't know who to listen to.

How I handled it was by putting them in contact with each other and making them aware of the conflict in a diplomatic manner.

What ended up happening was they agreed on what they wanted done, and I was able to continue with my job.

## Problem with coworker

In a previous job, I faced a challenging situation with a collegue due to a misunderstanding about project responsabilities. To resolve it, I iniatiated a calm and open conversation, expressing my perspective while also listening to therir concerns. We are able to clear up the misunderstanding and agreed on a better way to communicate and collaborate moving forward. This experience taught me the importance of addresing issues early and maintaining open, respectfull comunications

# Personal life

Hello, I'm S----. I'm married and have two wonderful children. My elder daughter is 6 years old, and my youngest son is just 7 months old. Being a family man is one of the most rewarding experiences of my life. Every day is a new adventure with them, from playing with my daughter to enjoying the smiles and babbling of my baby boy.

My passion for technology has been with me for as long as I can remember. I love staying updated on the latest innovations and gadgets. I'm always looking for ways to incorporate technology in a positive way into my family's life, whether it's using educational apps for my daughter o

Balancing my family life with my passion for technology can be quite a challenge, but it's one I'm eager to tackle with enthusiasm every day!

I have been married for 7 years

In my free time, I watch movies, play with my kids, and go to the mall or to a restaurant.And sometimes I play soccer with friends

# Superheroe favorito

Superman is definitely my favorite superhero, and there are several reasons why. First and foremost, I admire his unwavering dedication to justice and truth. His moral compass is as strong as his superpowers, and that's something I truly admire.

what really sets him apart is how he uses those powers for the betterment of humanity. Instead of dominating others or seeking personal gain, he selflessly protects the innocent and fights for peace and justice.

In essence, Superman embodies the ideals of hope, courage, and compassion. He inspires us to be the best versions of ourselves and to stand up for what is right, no matter the odds. For these reasons and more, Superman will always be my favorite superhero.
__________


 BADGE cloud practitioner https://www.credly.com/badges/06ab54c5-a0c5-4d5d-b98a-217e91049eab
# ROUTINE

- Review the current infrastructure status on AWS, Kubernetes clusters, and other cloud platforms. Check monitoring dashboards (e.g., Grafana, Prometheus) for any anomalies or performance issues.

- Work on infrastructure improvements and optimizations. Update Terraform configurations to automate provisioning and ensure scalability of resources. Investigate and resolve any infrastructure-related tickets raised by the team.

- Dive into CI/CD pipeline configurations. Review Jenkins jobs and pipelines to ensure smooth integration and deployment processes. Troubleshoot any failed builds or deployment issues and implement necessary fixes.
__________

# Scripts
I have made scripts that connect to mongo for queries, inserts and delete.

also scripts that connect to a kubernetes cluster to validate pod status and do automatic restarts.

Script that allow to delete logs searching by creation date


I using Terraform to create virtual networks, EC2 instances, Auto Scaling groups, and configuring load balancers to ensure high availability and scalability according to demand.

Additionally, I have worked on implementing serverless architectures using services like AWS Lambda, API Gateway, and DynamoDB to develop highly available and cost-effective serverless applications.


# Projects

One of my standout projects involved implementing a continuous delivery platform using Jenkins and Kubernetes. We configured Jenkins pipelines to automatically build, test, and deploy our applications to Kubernetes clusters. We utilized Helm to manage deployments and infrastructure as code to ensure consistency and replicability across different environments.

Another project I contributed to was the implementation of a monitoring on Kubernetes. We used Prometheus and Grafana to collect metrics and visualize the performance of our applications
We identified key metrics we wanted to monitor, such as CPU usage, memory utilization, and request latency. We set up alerts in Grafana to detect abnormal conditions or potential issues in our infrastructure or applications. These alerts were integrated with notification systems such as Pagerduty or teams so that the operations team could respond promptly to any problems.

In this project, our goal was to implement cloud infrastructure in AWS in an automated and controlled manner using Terraform, leveraging GitHub Actions for Continuous Integration and Continuous Delivery (CI/CD). We used Terraform to define all our infrastructure as code, including networks, EC2 instances, security groups, databases, and other resources needed for our applications on AWS. This code-based definition allowed us to manage and version our infrastructure efficiently and reproducibly. We configured GitHub Actions workflows to automatically trigger in response to changes in our source code repository. These workflows ran automated tests, such as unit and integration tests, on our Terraform-defined infrastructure to ensure its integrity and correct operation. : We implemented a continuous delivery flow that used GitHub Actions to automate the deployment of our infrastructure in AWS. When changes were made to the Terraform code, GitHub Actions automatically executed Terraform commands to apply those changes to our infrastructure in AWS. e used GitHub Actions' secret management capabilities to securely store credentials and other sensitive variables needed to deploy and manage our infrastructure in AWS.

"The QA team was facing issues with having consistent data for their tests, and much of the information created afterward was not reused, which resulted in the database having a lot of unnecessary data. What was proposed was a TDM (Test Data Management) system where another database was created, bringing in production data via Optim, which masked the sensitive data. Additionally, a Java program was developed to create other data that, due to its nature, couldn't be sourced from production. With this data, views were created in the database to group the data according to specific business cases, such as users with certain characteristics and products. Using these views and information, two pipelines were created in Azure DevOps. One pipeline deployed the infrastructure as code, consisting of an API Gateway, a Lambda function, and a DynamoDB where the views and the available data quantity for lending were stored. QA users, in their testing pipelines, could add two tasks that were developed in TypeScript. In one task, they could specify the view name, the amount of data to be lent, and how long they would use this data. The other task was used to release the lent data once testing was completed. The workflow was that the QA teams would input the necessary information into this task, which would then call the API Gateway, triggering a Lambda function. This function would retrieve the view key, check if there was a sufficient amount of users, and then call a JAR, which would copy the data into the database where the QA team was conducting their tests."  

I worked on a project that involved managing a high-traffic e-commerce web application that required a scalable, secure, and highly available architecture on AWS. To achieve this, I implemented the following services:

EC2 with ASG, Load Balancers, and Launch Configurations:

I set up Auto Scaling Groups (ASG) for EC2 instances to automatically adjust the number of instances based on the application demand. This optimized costs and allowed the application to scale during traffic spikes.
I implemented an Application Load Balancer (ALB) to distribute incoming traffic evenly across the EC2 instances, improving availability and ensuring a seamless user experience.
I created Launch Configurations that specified instance types, AMIs, network, and security settings, ensuring that each instance replicated the exact configuration within the Auto Scaling Group.
VPC (Virtual Private Cloud):

I designed a VPC with public and private subnets. EC2 instances in private subnets were protected from external access, while public subnets hosted exposed services, such as the load balancer.
Configured route tables and NAT gateways so that private instances could access the internet for specific tasks, like software updates, enhancing the security of the infrastructure.
RDS (Relational Database Service):

I used Amazon RDS (PostgreSQL) to handle transactional databases. RDS provided automatic backups and Multi-AZ replication, ensuring high availability and disaster recovery for the application’s critical data.
Redis:

I implemented Amazon ElastiCache for Redis as a caching system to reduce latency and improve performance. Redis was used to store user sessions and frequently accessed data, speeding up data retrieval and offloading the primary database.
Route 53:

I configured Route 53 for traffic routing, taking advantage of failover capabilities to automatically redirect users to backup environments in case of an issue in the primary availability zone. Additionally, I managed the application’s DNS records and configured necessary CNAME and A records for the required subdomains.
Secrets Manager:

To securely manage credentials and API keys, I used AWS Secrets Manager. This allowed me to store and retrieve secrets for the database, API keys, and authentication tokens securely. I also set up periodic credential rotation to ensure sensitive information remained protected.
Systems Manager:

With AWS Systems Manager, I configured parameters and managed remote access to EC2 instances without opening SSH ports on the firewall. I also used Systems Manager to run commands and patch instances, automating security maintenance and optimizing infrastructure management.
S3:

I used Amazon S3 to store static files like images and user documents and set up access policies to ensure only authorized users could access these resources. Additionally, I implemented S3 Lifecycle Policies to archive older data in S3 Glacier, optimizing storage costs.
_________________________


in the projects I have worked on, I have not used this functionality/tool.


____________
yes I have worked with gitlab CI /Cd but about a year ago because the project I am working on is using githubaction.

yes i know elasticsearch but i have not worked with it in productive environments



_____________
# quality

I am a fast learner, dedicated and goal oriented.

__________________
# CURRENT JOB
•	Managed and maintained Kubernetes clusters for high-availability applications, ensuring system scalability and reliability.
•	Gained exposure to Prometheus/Alertmanager/Grafana for monitoring and alerting, enhancing system health and performance visibility.
•	Developed and maintained automation scripts in Bash and Python for infrastructure tasks and application deployment.
•	Implemented Infrastructure as Code (IaC) using Terraform to improve environment reproducibility and consistency.
•	Participated in incident handling and on-call rotation, demonstrating readiness to address critical issues and ensure system availability.


____________

_________________

One aspect I've identified for improvement is my public speaking skills. While I'm comfortable communicating ideas one-on-one or in small groups, I have occasionally felt nervous when speaking in front of larger audiences. However, I've been actively addressing this by participating in public speaking workshops and seeking opportunities to practice presenting in various settings. I believe that with continued effort and experience, I can further strengthen this skill and become a more confident presenter
# to improve
"I've found that I can be overly critical of my own work at times, which can occasionally slow down my progress. While I believe in striving for excellence, I've learned that perfectionism can sometimes hinder efficiency. To address this, I've been working on finding the right balance between attention to detail and timely completion of tasks. I've also been seeking feedback from colleagues to gain perspective and ensure that I'm meeting expectations without getting bogged down in unnecessary revisions."
_______________________
# questions

- could you explain to me how the payment would be made in dollars?
- could you explain to me how the payment of health and pension contributions would be made?
- if the project I join ends, how long can I be without assignment in another project?
- on holidays in colombia do i have to work?
- could you explain to me the company's career plan?
- can you tell me the type of contract
- How does the process continue?"
- For how long is the contract?

__________________
service mesh
son solo azure
se usa helm
CI/CD?
SCRUM, KANBAN O
tema de seguridad
git strategy
cantidad de despliegues


_____________________
DevOps is a work culture primarily centered around collaboration, communication, and integration among the development teams

What are the key benefits of using DevOps?

The key benefits of using DevOps include faster time-to-market, increased collaboration between teams, improved software quality and reliability, and efficient use of resources.

Continuous integration (CI) is the process of automating and integrating code changes and updates from many team members during software development. In CI, automated tools confirm that software code is valid and error-free before it's integrated, which helps detect bugs and speed up new releases.


Continuous Delivery focuses on ensuring that software is deliverable at any time and in any environment, Continuous Deployment takes this concept a step further by automating the deployment of changes to production as soon as they are ready.

Continuos delivery is keeping the code in a deployable state Continuos deployment is actually doing the deployment frequently

What are the differences between Continuous Integration, Continuous Delivery, and Continuous Deployment?

Continuous Integration (CI) is a DevOps software development practice that permits developers to combine/merge the changes to their code in the central repository to run automated builds and tests

Continuous Delivery (CD) refers to the building, testing, and delivering improvements to the software code. The most critical part of the CD is that the code is always in a deployable stat

Continuous Deployment (CD) is the ultimate stage in the DevOps pipeline. It  refers to automatic release of any developer changes from the repository to the production stage.

Idempotency is the property that ensures that the results from an operation are the same, even if the same function is applied multiple times beyond the initial application. 

Agile prescribes working incrementally, collaboratively and flexibly; it does not prescribe a specific framework or methodology. A few of the most popular frameworks that Agile teams adopt are Scrum, Kanban and Extreme Programming. Teams may choose one of these frameworks or pieces of each.
## ADVANTAGES CLOUD COMPUTING

- trade capital expenses for variable expenses
- benefits from massive economic of scale
- stop guessing capacity
- increase speed and agility
- go global in minutes

## RTO and RPO
 Recovery time objective (RTO): The maximum acceptable delay between the interruption of service and restoration of service. This determines an acceptable length of time for service downtime.
Recovery point objective (RPO): The maximum acceptable amount of time since the last data recovery point. This determines what is considered an acceptable loss of data.

# branch stategy 


## GIT FLOW
In the Git flow development model, you have one main development branch with strict access to it. It’s often called the develop branch.

Developers create feature branches from this main branch and work on them. Once they are done, they create pull requests. In pull requests, other developers comment on changes and may have discussions, often quite lengthy ones

**Overview:** Utilizes two main branches: master for production-ready code and develop for ongoing development. Feature branches, release branches, and hotfix branches are used for specific tasks.

**Advantages:**

- Structured Release Management: Provides a structured approach to release management, with dedicated branches for features, releases, and hotfixes.
- Stability: Keeps the master branch stable, as it only contains production-ready code, while ongoing development occurs in the develop branch.
- Parallel Development: Allows for parallel development of features and hotfixes, ensuring that ongoing development is not blocked by release activities.

**Disadvantages:**
- Complexity: The Gitflow model can be relatively complex, especially for smaller teams or projects, requiring discipline and adherence to the workflow.

- Overhead: Managing multiple long-lived branches (e.g., feature, release, hotfix) can introduce overhead and administrative burden, potentially slowing down development.



## Trunk-based Development Workflow

**Overview:** All development occurs directly on the main branch (master or main). Developers use feature toggles, feature flags, or experimental branches to isolate incomplete or experimental features.

**Advantages:** Promotes continuous integration and rapid feedback cycles, reduces merge conflicts, and encourages small, frequent commits.


- Simplicity: Trunk-based development simplifies the branching model by having all development occur directly on the main branch (often master or main). This simplicity can reduce overhead and streamline the development process, especially for small teams or projects.

- Continuous Integration: Trunk-based development promotes continuous integration practices, where developers frequently merge their changes into the main branch. This ensures that code changes are regularly integrated, tested, and validated, leading to fewer integration issues and faster feedback cycles.

- Reduced Merge Conflicts: With developers working directly on the main branch, there are fewer long-lived feature branches, reducing the likelihood of merge conflicts. Developers are encouraged to make small, incremental changes, minimizing the impact of conflicts and making them easier to resolve.

- Faster Feedback: Trunk-based development encourages rapid feedback cycles, as changes are integrated into the main branch and tested promptly. This enables developers to identify issues early and address them quickly, improving overall code quality and stability.

- Encourages Collaboration: Since all development occurs on the main branch, developers have visibility into each other's changes and can collaborate more effectively. Code reviews, discussions, and feedback are integral parts of the development process, fostering collaboration and knowledge sharing within the team.

**Disadvantages:**

- Risk of Instability: Working directly on the main branch can introduce instability, especially if developers introduce bugs or incomplete features. Since changes are immediately reflected in the main branch, there is a risk of disrupting other developers' work or impacting the stability of the codebase.

- Limited Experimentation: Trunk-based development may discourage experimentation or exploration of new ideas, as changes are immediately integrated into the main branch. Developers may feel pressured to prioritize stability over innovation, potentially stifling creativity and innovation.

- Dependency Management: Trunk-based development relies heavily on feature toggles or experimental branches to isolate incomplete or experimental features. Managing these toggles or branches effectively requires careful coordination and communication among team members, adding complexity to the development process.

- High Level of Discipline: Trunk-based development requires a high level of discipline and adherence to best practices such as continuous integration, automated testing, and small, incremental changes. Without proper discipline, the main branch can quickly become unstable, leading to integration issues and delays.

**Example:** Developers work directly on the main branch, using feature toggles or experimental branches to isolate incomplete features until they are ready for release.

## Feature Branch:

**Overview:** Each new feature or task is developed in its own branch, typically branched off the main development branch (e.g., master or develop).

**Advantages:**

- Isolation: Each feature or task is developed in its own branch, allowing for isolation of changes and preventing interference with other features.
- Collaboration: Facilitates collaborative development by enabling team members to work on different features simultaneously without conflicts.
- Code Review: Pull requests can be used for code review, providing an opportunity for feedback and ensuring quality before merging.

**Disadvantages:**

- Branch Proliferation: If not managed properly, the repository can become cluttered with numerous feature branches, making it challenging to track and manage.
- Integration Complexity: Merging feature branches back into the main branch (e.g., master or develop) can sometimes result in merge conflicts or integration issues.

Example: GitHub Flow, where feature branches are created for each new feature, and pull requests are used for code review and merging.


#  Deloyment Startegies

## recreate: terminate the old version and release the new one
    - Pros:
        - Easy to setup.
        - Application state entirely renewed
    - Cons:
        - High impact on the user, expect downtime that depends on both shutdown and boot duration of the application.
## Rolling Deployment
A rolling deployment is a deployment strategy that updates running instances of an application with the new release. All nodes in a target environment are incrementally updated with the service or artifact version in integer N batches.

## Blue-green deployment 
is a deployment strategy that utilizes two identical environments, a “blue” (aka staging) and a “green” (aka production) environment with different versions of an application or service. Quality assurance and user acceptance testing are typically done within the blue environment that hosts new versions or changes. User traffic is shifted from the green environment to the blue environment once new changes have been testing and accepted within the blue environment.

## Canary Deployment

A canary deployment is a deployment strategy that releases an application or service incrementally to a subset of users. All infrastructure in a target environment is updated in small phases (e.g: 2%, 25%, 75%, 100%). A canary release is the lowest risk-prone, compared to all other deployment strategies, because of this control.

## A/B
In A/B testing, different versions of the same service run simultaneously as “experiments” in the same environment for a period of time. Experiments are either controlled by feature flags toggling, A/B testing tools, or through distinct service deployments. It is the experiment owner’s responsibility to define how user traffic is routed to each experiment and version of an application. Commonly, user traffic is routed based on specific rules or user demographics to perform measurements and comparisons between service versions. Target environments can then be updated with the optimal service version.

# Containerization
Containerization is a software deployment process that bundles an application’s code with all the files and libraries it needs to run on any infrastructure.

A container is a standard unit of software bundled with dependencies so that applications can be deployed fast and reliably b/w different computing platforms.

## Containerization compared to virtual machines
Containerization is a similar but improved concept of a VM. Instead of copying the hardware layer, containerization removes the operating system layer from the self-contained environment. This allows the application to run independently from the host operating system. Containerization prevents resource waste because applications are provided with the exact resources they need. 

# Docker
Docker, or Docker Engine, is a popular open-source container runtime that allows software developers to build, deploy, and test containerized applications on various platforms. Docker containers are self-contained packages of applications and related files that are created with the Docker framework.

1. Can you tell something about docker container?

In simplest terms, docker containers consist of applications and all their dependencies.
They share the kernel and system resources with other containers and run as isolated systems in the host operating system.
The main aim of docker containers is to get rid of the infrastructure dependency while deploying and running applications. This means that any containerized application can run on any platform irrespective of the infrastructure being used beneath.
Technically, they are just the runtime instances of docker images.

2. What is the difference between an Image, Container, and Engine?

- An image in Docker is a lightweight, stand-alone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files.

- A container is a running instance of an image. It is a lightweight, standalone, and executable software package that includes everything needed to run the software in an isolated environment.

- A Docker engine is a background service that manages and runs Docker containers. It is responsible for creating, starting, stopping, and deleting containers, as well as managing their networking and storage. The Docker engine is the underlying technology that runs and manages the containers.

3. What is a DockerFile?

It is a text file that has all commands which need to be run for building a given image.

4. Can you tell what is the functionality of a hypervisor?

A hypervisor is a software that makes virtualization happen because of which is sometimes referred to as the Virtual Machine Monitor. This divides the resources of the host system and allocates them to each guest environment installed.

5. What can you tell about Docker Compose?

It is a YAML file consisting of all the details regarding various services, networks, and volumes that are needed for setting up the Docker-based application. So, docker-compose is used for creating multiple containers, host them and establish communication between them. For the purpose of communication amongst the containers, ports are exposed by each and every container.

6.  Can you tell something about docker namespace?

A namespace is basically a Linux feature that ensures OS resources partition in a mutually exclusive manner. This forms the core concept behind containerization as namespaces introduce a layer of isolation amongst the containers. In docker, the namespaces ensure that the containers are portable and they don't affect the underlying host. Examples for namespace types that are currently being supported by Docker – PID, Mount, User, Network, IPC.

7. What is the docker command that lists the status of all docker containers?

In order to get the status of all the containers, we run the below command: docker ps -a

8. On what circumstances will you lose data stored in a container?

The data of a container remains in it until and unless you delete the container.

9. What is docker image registry?

A Docker image registry, in simple terms, is an area where the docker images are stored. Instead of converting the applications to containers each and every time, a developer can directly use the images stored in the registry.
This image registry can either be public or private and Docker hub is the most popular and famous public registry available.

10. How many Docker components are there?

There are three docker components, they are - Docker Client, Docker Host, and Docker Registry.

- Docker Client: This component performs “build” and “run” operations for the purpose of opening communication with the docker host.
- Docker Host: This component has the main docker daemon and hosts containers and their associated images. The daemon establishes a connection with the docker registry.
- Docker Registry: This component stores the docker images. There can be a public registry or a private one. The most famous public registries are Docker Hub and Docker Cloud.

11. What is a Docker Hub?

It is a public cloud-based registry provided by Docker for storing public images of the containers along with the provision of finding and sharing them.
The images can be pushed to Docker Hub through the docker push command.

12. What command can you run to export a docker image as an archive?

This can be done using the docker save command and the syntax is: docker save -o <exported_name>.tar <container-name>

13. What command can be run to import a pre-exported Docker image into another Docker host?

This can be done using the docker load command and the syntax is docker load -i <export_image_name>.tar

14. Can a paused container be removed from Docker?

No, it is not possible! A container MUST be in the stopped state before we can remove it.

15. What command is used to check for the version of docker client and server?

The command used to get all version information of the client and server is the docker version.
To get only the server version details, we can run docker version --format '{{.Server.Version}}'

16. Differentiate between virtualization and containerization.

Virtualization 	
- This helps developers to run and host multiple OS on the hardware of a single physical server.
- Hypervisors provide overall virtual machines to the guest operating systems.
- These virtual machines form an abstraction of the system hardware layer this means that each virtual machine on the host acts like a physical machine.
Containerization

- This helps developers to deploy multiple applications using the same operating system on a single virtual machine or server.
- Containers ensure isolated environment/ user spaces are provided for running the applications. Any changes done within the container do not reflect on the host or other containers of the same host.
- Containers form abstraction of the application layer which means that each container constitutes a different application.

3. Can a container restart by itself?
Yes, it is possible only while using certain docker-defined policies while using the docker run command. Following are the available policies:

    1. Off: In this, the container won’t be restarted in case it's stopped or it fails.
    2. On-failure: Here, the container restarts by itself only when it experiences failures not associated with the user.
    3. Unless-stopped: Using this policy, ensures that a container can restart only when the command is executed to stop it by the user.
    4. Always: Irrespective of the failure or stopping, the container always gets restarted in this type of policy.

These policies can be used as:
docker run -dit — restart [restart-policy-value] [container_name]

4. Can you tell the differences between a docker Image and Layer?

Image: This is built up from a series of read-only layers of instructions. An image corresponds to the docker container and is used for speedy operation due to the caching mechanism of each step.

Layer: Each layer corresponds to an instruction of the image’s Dockerfile. In simple words, the layer is also an image but it is the image of the instructions run.

- The result of building this docker file is an image. Whereas the instructions present in this file add the layers to the image. The layers can be thought of as intermediate images.

6. Where are docker volumes stored in docker?

Volumes are created and managed by Docker and cannot be accessed by non-docker entities. They are stored in Docker host filesystem at /var/lib/docker/volumes/

7. What does the docker info command do?

The command gets detailed information about Docker installed on the host system. The information can be like what is the number of containers or images and in what state they are running and hardware specifications like total memory allocated, speed of the processor, kernel version, etc.

8. Can you tell the what are the purposes of up, run, and start commands of docker compose?

Using the up command for keeping a docker-compose up (ideally at all times), we can start or restart all the networks, services, and drivers associated with the app that are specified in the docker-compose.yml file. Now if we are running the docker-compose up in the “attached” mode then all the logs from the containers would be accessible to us. In case the docker-compose is run in the “detached” mode, then once the containers are started, it just exits and shows no logs.
Using the run command, the docker-compose can run one-off or ad-hoc tasks based on the business requirements. Here, the service name has to be provided and the docker starts only that specific service and also the other services to which the target service is dependent (if any).
- This command is helpful for testing the containers and also performing tasks such as adding or removing data to the container volumes etc.
Using the start command, only those containers can be restarted which were already created and then stopped. This is not useful for creating new containers on its own.

10. Can you tell the approach to login to the docker registry?

Using the docker login command credentials to log in to their own cloud repositories can be entered and accessed.

11. List the most commonly used instructions in Dockerfile?

- FROM: This is used to set the base image for upcoming instructions. A docker file is considered to be valid if it starts with the FROM instruction.
- LABEL: This is used for the image organization based on projects, modules, or licensing. It also helps in automation as we specify a key-value pair while defining a label that can be later accessed and handled programmatically.
- RUN: This command is used to execute instructions following it on the top of the current image in a new layer. Note that with each RUN command execution, we add layers on top of the image and then use that in subsequent steps.
- CMD: This command is used to provide default values of an executing container. In cases of multiple CMD commands the last instruction would be considered.

12. Can you differentiate between Daemon Logging and Container Logging?

In docker, logging is supported at 2 levels and they are logging at the Daemon level or logging at the Container level.

- Daemon Level: This kind of logging has four levels- Debug, Info, Error, and Fatal.
    - Debug has all the data that happened during the execution of the daemon process.
    - Info carries all the information along with the error information during the execution of the daemon process.
    - Errors have those errors that occurred during the execution of the daemon process.
    - Fatal has the fatal errors that occurred during the execution.
Container Level:
    - Container level logging can be done using the command: sudo docker run –it <container_name> /bin/bash
    - In order to check for the container level logs, we can run the command: sudo docker logs <container_id>
    13. What is the way to establish communication between docker host and Linux host?
    This can be done using networking by identifying the “ipconfig” on the docker host. This command ensures that an ethernet adapter is created as long as the docker is present in the host.

14. What is the best way of deleting a container?
We need to follow the following two steps for deleting a container:
- docker stop <container_id>
- docker rm <container_id>

15. Can you tell the difference between CMD and ENTRYPOINT?

CMD command provides executable defaults for an executing container. In case the executable has to be omitted then the usage of ENTRYPOINT instruction along with the JSON array format has to be incorporated.
ENTRYPOINT specifies that the instruction within it will always be run when the container starts. 
This command provides an option to configure the parameters and the executables. If the DockerFile does not have this command, then it would still get inherited from the base image mentioned in the FROM instruction.
- The most commonly used ENTRYPOINT is /bin/sh or /bin/bash for most of the base images.
As part of good practices, every DockerFile should have at least one of these two commands.

2. How many containers you can run in docker and what are the factors influencing this limit?

There is no clearly defined limit to the number of containers that can be run within docker. But it all depends on the limitations - more specifically hardware restrictions. The size of the app and the CPU resources available are 2 important factors influencing this limit. In case your application is not very big and you have abundant CPU resources, then we can run a huge number of containers.

3. Describe the lifecycle of Docker Container?

The different stages of the docker container from the start of creating it to its end are called the docker container life cycle. 
The most important stages are:

- Created: This is the state where the container has just been created new but not started yet.
- Running: In this state, the container would be running with all its associated processes.
- Paused: This state happens when the running container has been paused.
- Stopped: This state happens when the running container has been stopped.
- Deleted: In this, the container is in a dead state.

![alt text](image-24.png)



### run and cmd

The CMD and RUN commands in Docker are used to specify commands that should be executed when a container is started from a given image.

The CMD command, is used to specify the default command that should be executed when a container is started from an image. This command can be overridden when starting a container, which means that it does not have to be executed every time a container is started.

The RUN command, on the other hand, is used to execute a command during the image-building process. It will run command(s) in a new layer on top of the current image and commit the results. The command(s) in a RUN instruction will always be executed when the image is being built.

### ADD and COPY

Both the commands have similar functionality, but COPY is more preferred because of its higher transparency level than that of ADD.
COPY provides just the basic support of copying local files into the container whereas ADD provides additional features like remote URL and tar extraction support.


### CMD and entrypoint
Because CMD and ENTRYPOINT work in tandem, they can often be confusing to understand. However, they have different effects and exist to increase your image’s flexibility: ENTRYPOINT sets the process to run, while CMD supplies default arguments to that process.

The ENTRYPOINT Dockerfile instruction sets the process that’s executed when your container starts.

ENTRYPOINT is the process that’s executed inside the container.
CMD is the default set of arguments that are supplied to the ENTRYPOINT process.
There are also differences in how you override these values when you start a container:

CMD is easily overridden by appending your own arguments to the docker run command.
ENTRYPOINT can be changed using the --entrypoint flag, but this should rarely be necessary for container images being used in the way they were intended. If you do change the ENTRYPOINT, you’ll almost certainly need to set a custom CMD too—as otherwise, your new ENTRYPOINT is likely to receive arguments that it doesn’t understand.

### ARG and ENV

ARG:

ARG is used to define build-time variables.
These variables are only accessible during the image build process, not during the container runtime.
They are typically used to pass information to the Dockerfile from the outside at build time.
Common use cases include passing version numbers, file paths, or any other information that might change during the build process.
You can pass ARG values via the docker build command using the --build-arg flag.

```
ARG VERSION=latest
FROM ubuntu:${VERSION}

```
Then, you can build the Docker image with a specific version:

```
docker build --build-arg VERSION=20.04 .
```
NV:

ENV is used to set environment variables that are available during the container runtime.
These variables can be accessed by processes running inside the container.
They are typically used to configure software inside the container, such as setting paths, defining default configurations, or providing credentials.
ENV values can be hardcoded within the Dockerfile or passed dynamically during container runtime.

```
FROM ubuntu
ENV PATH="/usr/local/bin:${PATH}"
```
Here, we're adding /usr/local/bin to the PATH environment variable inside the container.

In summary, ARG is used for build-time variables, while ENV is used for runtime environment variables inside the container.

### Docker python 

```
 Utilizamos una imagen oficial de Python como base
FROM python:3.9-slim

# Establecemos el directorio de trabajo en /app
WORKDIR /app

# Copiamos el archivo de requisitos a la imagen
COPY requirements.txt requirements.txt

# Instalamos las dependencias
RUN pip install --no-cache-dir -r requirements.txt

# Copiamos solo el archivo ejecutable de Python a la imagen
COPY app.py .

# Configuramos un usuario no root para la imagen por motivos de seguridad
RUN useradd appuser && chown -R appuser /app
USER appuser

# Exponemos el puerto 8080 para que la aplicación sea accesible desde fuera del contenedor
EXPOSE 8080

# Comando por defecto para ejecutar la aplicación
CMD ["python", "app.py"]

```

### A multistage Docker build 

is a technique used to optimize Docker images by using multiple stages in the Dockerfile. This allows you to separate different stages of the build process, reducing the final image size and improving build performance.

example of a Python Dockerfile with a multistage build:

```
# Stage 1: Build stage
FROM python:3.9-slim AS build-stage

# Set the working directory in the container
WORKDIR /app

# Copy the requirements file and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy the Python application code
COPY . .

# Stage 2: Production stage
FROM python:3.9-slim AS production-stage

# Set the working directory in the container
WORKDIR /app

# Copy only the necessary files from the build stage
COPY --from=build-stage /app /app

# Expose the port on which the application will run (if needed)
# EXPOSE 8000

# Define the command to run the Python application
CMD ["python", "app.py"]

```

### python optimized
```
# Stage 1: Build stage
FROM python:3.9-slim AS build-stage

# Set the working directory in the container
WORKDIR /app

# Copy the Python application code
COPY . .

# Install dependencies and create a virtual environment
RUN python -m venv /venv
RUN /venv/bin/pip install --no-cache-dir -r requirements.txt

# Stage 2: Production stage
FROM python:3.9-slim AS production-stage

# Set the working directory in the container
WORKDIR /app

# Copy only the virtual environment and the necessary files from the build stage
COPY --from=build-stage /venv /venv
COPY --from=build-stage /usr/local/bin/python /usr/local/bin/python
COPY --from=build-stage /usr/local/lib/python3.9 /usr/local/lib/python3.9

# Define the command to run the Python application
CMD ["/venv/bin/python", "app.py"]
```
In the build stage, we create a virtual environment (/venv) and install the Python dependencies inside it. Then we copy the entire application code into the image.

In the production stage, we copy only the virtual environment (/venv) and necessary Python executable and library files from the build stage into the production image. This includes the Python executable (/usr/local/bin/python) and the Python standard library (/usr/local/lib/python3.9).

Finally, we define the command to run the Python application using the Python executable from the virtual environment (/venv/bin/python).

With this setup, only the Python executable and necessary library files are copied into the production image, reducing its size and ensuring that only the essential components are included.




# GIT

1. Explain the difference between rebasing and merge in Git?

-  Git rebase is a command that allows developers to integrate changes from one branch to another.
- Git merge is a command that allows you to merge branches from Git.

Git rebase and merge both integrate changes from one branch into another. Where they differ is how they used. Git rebase moves a feature branch into a master. Git merge adds a new commit, preserving the history.

2. Have you faced the situation where you resolve conflicts in Git? How?

A merge conflict is an event that takes place when Git is unable to automatically resolve differences in code between two commits. Git can merge the changes automatically only if the commits are on different lines or branches. Here are the steps that will help you resolve conflicts in Git:

    1. The easiest way to resolve a conflicted file is to open it and make any necessary changes
    2. After editing the file, we can use the git add a command to stage the new merged content
    3. The final step is to create a new commit with the help of the git commit command
    4. Git will create a new merge commit to finalize the merge

3. How to revert a commit that has already been pushed and made public?

There are two processes through which you can revert a commit:
    1. Remove or fix the bad file in a new commit and push it to the remote repository. Then commit it to the remote repository using:
    git commit –m “commit message”
    2. Create a new commit to undo all the changes that were made in the bad commit. Use the following command:
    git revert <commit id>

4. Tell about the commands git reset — mixed and git merge — abort?.

git reset — mixed is used to undo changes made in the working directory and staging area.

git merge — abort helps stop the merge process and return back to the state before the merging began.

5. How will you find a list of files that has been modified in a particular commit?

The command to get a list of files that has been changed in a particular commit is:
git diff-tree –r {commit hash}
• -r flag allows the command to list individual files
• commit hash lists all the files that were changed or added in the commit.

6. How will you fix a broken commit? What command you will use?

To fix a broken commit in Git, We use the “git commit — amend” command, which helps us combine the staged changes with the previous commits instead of creating a fresh new commit.

7. Explain git stash drop?

Git ‘stash drop’ command is used to remove the stashed item. This command will remove the last added stash item by default, and it can also remove a selected item as well.
Ex: If you want to delete item named stash@{manoj}; you can use the command:
git stash drop stash@{manoj}.

git stash is a useful tool for handling temporary changes in your working directory in Git. It allows you to quickly move between tasks without losing your work in progress.

8. Explain about “git cherry-pick”?

This command enables you to pick up commits from a branch within a repository and apply it to another branch. This command is useful to undo changes when any commit is accidentally made to the wrong branch. Then, you can switch to the correct branch and use this command to git cherry-pick the commit.

9. Can you tell the difference between git pull and git fetch?

Git pull command pulls new changes or commits from a particular branch from your central repository and updates your target branch in your local repository. (Git pull = git fetch + git merge)

Git fetch is also used for the same purpose but it works in a slightly different way. When you perform a git fetch, it pulls all new commits from the desired branch and stores it in a new branch in your local repository. If you want to reflect these changes in your target branch, git fetch must be followed with a git merge.

10. What is origin in Git?

Origin refers to the remote repository that a project was originally cloned from and is used instead of the original repository’s URL.

11. What is the difference between resetting and reverting?

git reset changes the state of the branch to a previous one by removing all of the states after the desired commit,

git revert does it through the creation of new reverting commits and keeping the original one intact.

12. What is ‘staging area’ or ‘index’ in Git?

That before completing the commits, it can be formatted and reviewed in an intermediate area known as ‘Staging Area’ or ‘Index’. Every change is first verified in the staging area and then that change is committed to the repository.

![alt text](image-19.png)

13. What work is restored when the deleted branch is recovered?

The files which were stashed and saved in the stash index list will be recovered back. Any untracked files will be lost. Also, it is a good idea to always stage and commit your work or stash them.

14. What is Head in Git?

Git maintains a variable for referencing, called HEAD to the latest commit in the recent checkout branch. So if we make a new commit in the repo then the pointer or HEAD is going to move or change its position to point to a new commit.

15. What is the purpose of branching and its types?

It allows the user to switch between the branches to keep the current work in sync without disturbing master branches and other developer’s work as per their requirements.

· Feature branching — A feature branch model keeps all of the changes for a particular feature inside of a branch. When the feature is fully tested and validated by automated tests, the branch is then merged into master.

· Task branching — In this branch, each task is implemented on its own branch with the task key included in the branch name. It is easy to see which code implements which task, just look for the task key in the branch name.

· Release branching — Once the develop branch has acquired enough features for a release, you can clone that branch to form a Release branch. Creating this branch starts the next release cycle, so no new features can be added after this point, only bug fixes, documentation generation, and other release-oriented tasks should go in this branch. Once it is ready to ship, the release gets merged into master and tagged with a version number.

## Basic Git Commands — Refresh your mind once again

git init: creating a new repository.

git clone: to copy or check out the working repository.

git pull: fetch the code already in the repository.

git push: sending the changes to the master branch.

git add: It adds file changes in an existing directory to index.

git commit –m [type in a message] — It is used to snapshot or record a file.

git diff [first branch] [second branch] — it is used to display the differences present between the two branches.

git rest [commit] — It is used to undo all the changes that have been incorporated as a part of a commit after a specified commit has taken place.

git reset –hard [commit] — This command is used to discard all the history and takes us to the last specified commit.

git log –follow [file] — his is similar to that of git log with the additional difference that it lists the version history for a particular file.

git show [commit] — This is used to display the metadata and all the content related changes of a particular commit.

git tag [commitID] — This is used to give particular tags to the code commits.

git branch [branch-name] — This is used to create a new branch.
git branch –d [branch name] — It is used to delete the current branch name specified.

git checkout [branch-name] — It is helpful in switching from one branch to another.

git status: To know the comparison between the working directories and index.

# Terraform


the primary purpose of terraform state is to store bindings between objects in a remote system and resource instances declared in your configuration

Import comand

import block terraform 1.5

importar recursos existentes a terraform

https://www.youtube.com/watch?v=znfh_00EDZ0

https://spacelift.io/blog/importing-exisiting-infrastructure-into-terraform

https://www.youtube.com/watch?v=JcV4nvG8vmU


azure: azure terrafy
AWS: terraformer, firefly (cost)

____________

## terraform import

Getting the pre-existing cloud resources under the Terraform management is facilitated by Terraform import. import is a Terraform CLI command which is used to read real-world infrastructure and update the state, so that future updates to the same set of infrastructure can be applied via IaC.

As discussed earlier, Terraform import does not generate the configuration files by itself. Thus, you need to create the corresponding configuration for the resource manually. This doesn’t need many arguments as we will have to add or modify them when we import the EC2 instance into our state file.

Think of it as if the cloud resource (EC2 instance) and its corresponding configuration were available in our files. All that’s left to do is to map the two into our state file. We do that by running the import command as follows.

```
terraform import aws_instance.myvm <Instance ID>
```

Please notice that the directory now also contains terraform.tfstate file. This file was generated after the import command was successfully run. Take a moment to go through the contents of this file.

Right now our configuration does not reflect all the attributes. Any attempt to plan/apply this configuration will fail because we have not adjusted the values of its attributes. To close the gap in configuration files and state files, run terraform plan and observe the output.

Improve config to avoid replacement
At this point, it is important to understand that the terraform.tfstate file is a vital piece of reference for Terraform. All of its future operations are performed with consideration for this state file. You need to investigate the state file and update your configuration accordingly so that there is a minimum difference between them.

The use of the word “minimum” is intentional here. Right now, you need to focus on not replacing the given EC2 instance but rather aligning the configuration so that the replacement can be avoided. Eventually, you would achieve a state of 0 difference.

Run terraform plan again, and observe the output.

If you have the same output (No changes), congratulations, as you have successfully imported a cloud resource into your Terraform config. It is now possible to manage this configuration via Terraform directly, without any surprises.

## import block (terraform 1.5)

In Terraform 1.5, a new import mechanism is available. A new top-level import block can be defined in your code to allow import operations. As this block is added in the code, import will not be a state operation, from now on, as for every other resource, it becomes a plannable operation. 

![alt text](image-23.png)

The import block, as you can see above, takes two parameters:

id → The id of the resource used in your cloud provider
to → The resource address that will be used in Terraform

Next, if you want to generate the configuration automatically, you can run the following command:

terraform plan -generate-config-out=generated_resources.tf


## terraform taint:

The terraform taint command is useful in scenarios where a resource becomes corrupted or is in an unknown state, and you want Terraform to manage it as if it were a new resource. By tainting the resource, you ensure that Terraform will recreate it during the next apply operation, bringing it back to a known and consistent state.

**Uso:**
```bash
terraform taint nombre_del_recurso
```

**Ejemplo:**
Supongamos que tenemos un recurso `aws_instance` que queremos marcar como "tainted":
```bash
terraform taint aws_instance.example
```

En resumen, `terraform taint` se utiliza para marcar recursos específicos como "tainted" y `terraform destroy` se utiliza para destruir todos los recursos administrados por Terraform según la configuración actual. Ambos comandos son útiles para administrar y mantener la infraestructura de manera eficiente y segura.

___________________
## Questions 
Q1: Suppose you created an ec2 instance with terraform and after creation, you have removed the entry from the state file now, when you run terraform apply what will happen?

As we have removed the entry from that state file so terraform will no longer manage that resource so on the next apply it will create a new resource.

Q2: What is a state file in Terraform?

A state file is a file in which Terraform keeps track of all the infrastructure that is deployed by it.

Q3: What is the best way to store the terraform state file?

The best way to store the state file is to keep it in the remote backend like S3 or GitLab-managed terraform state so, that whenever multiple people are working on the same code resource duplication won’t happen.

Q4: What is terraform state locking?

Whenever we are working on any terraform code and do terraform plan, apply or destroy terraform will lock the state file in order to prevent the destructive action.

Q5: What is Terraform backend?

A backend defines where Terraform stores its state data files. Terraform uses persisted state data to keep track of the resources it manages.

Q6: What is a plugin in Terraform?

The plugin is responsible for converting the HCL code into API calls and sends the request to the appropriate provider (AWS, GCP)

Q7: What is a null resource?


In Terraform, a null_resource is a resource that doesn't actually represent an infrastructure object like most other resources. Instead, it's a way to define a placeholder resource that can be used to execute arbitrary actions or commands outside of the typical resource lifecycle.

You can use a null_resource to define custom actions or tasks that are not directly supported by Terraform's built-in resource types. This can include running scripts, executing commands on a remote server, or interacting with external systems.

```
resource "null_resource" "example" {
  # Define triggers to force recreation of the null_resource
  triggers = {
    always_run = "${timestamp()}"
  }

  # Execute a local script
  provisioner "local-exec" {
    command = "echo Hello, Terraform!"
  }
}

```

Q8: What are the types of provisioners?

Remote exec: Run commands using Terraform on a remote server
Local exec: Run commands using Terraform on the local system

Q9: What is the use of Terraform module?

A Terraform module is a collection of standard configuration files in a dedicated directory. Terraform modules encapsulate groups of resources dedicated to one task, reducing the amount of code you have to develop for similar infrastructure components.

We can create the terraform modules one time and reuse them whenever needed
To make to code standardized
To reduce the code duplication
The module can be versioned

Q10: If I have created EC2 and VPC using Terraform and unfortunately tfstate file got deleted, can you recover it? (File is only on the local machine not on s3 or dynamo DB)

You can import the resources that are created by Terraform using terraform import command and then it will come to the state file

Q11: If we have created different-different modules like VPC, EC2, security group, access key, and subnet so how terraform will get an idea of which resource should deploy first?

Terraform automatically figures out the dependency graph based on the resource references in your code. It understands the relationships between resources, and it uses this information to determine the order in which the resources should be created or modified.
You can define the explicit dependency with the depends_on keyword

Q12: How I can delete/destroy specific resources without changing logic?

Using taint and destroy command
We need to taint that resource using terraform taint RESOURCE_TYPE.RESOURCE_NAME command
After tainting the resource, you can run the “destroy” command to remove the tainted resources using terraform destroy -target=RESOURCE_TYPE.RESOURCE_NAME command

Q13: How can we rename a resource in Terraform without deleting it?

We can rename a resource without deleting it using terraform mv command

The terraform mv command in Terraform is used to move and rename resources within your Terraform configuration files without actually modifying the infrastructure it manages. It allows you to update your Terraform configuration to reflect changes in the structure of your infrastructure, such as renaming resources or moving them to different locations, without causing Terraform to destroy and recreate the resources.

```
terraform mv aws_instance.example aws_instance.new_example

```

Q14: Let’s say you have created an EC2 instance using Terraform and someone does the manual change on it next time you run Terraform plan what will happen?

Terraform state will be mismatched and terraform will modify the EC2 instance to the desired state i.e. whatever we have defined in the .tf file

Q15: What is the difference between locals & variables in terraform?

The variables are defined in the variables.tf file or using variables keyword that can be overridden but the locals can not be overridden.
So if you want to restrict the overriding the variables at that time you need to use the locals.



# Ansible
In Ansible, modules are small programs written in Python that Ansible uses to perform specific tasks on managed systems. Modules can perform a wide variety of tasks, from package management and service configuration to file manipulation and user management.

In Ansible, roles are a way to organize and structure your playbooks and tasks into logical, reusable units. Roles in Ansible allow you to modularize system configuration and automation, making it easier to manage and maintain your infrastructure.



1. What is Ansible?
Ansible is an open-source platform that facilitates configuration management, task automation, or application deployment. It is a valuable DevOps tool. It was written in Python and powered by Red Hat. It uses SSH to deploy SSH without incurring any downtime.

2. List Ansible’s advantages
Ansible has many strengths, including:

- It’s agentless and only requires SSH service running on the target machines
- Python is the only required dependency and, fortunately, most systems come with the language pre-installed
- It requires minimal resources, so there’s low overhead
- It’s easy to learn and understand since Ansible tasks are written in YAML.
- Unlike other tools, most of which are Procedural, ansible is declarative; define the desired state, and Ansible fulfills the requirements needed to achieve it
- Ansible is an ideal tool for CI/CD processes, providing a stable infrastructure for provisioning the target environment and then deploying the application to it.

3. Describe how Ansible works.
ansible is broken down into two types of servers: controlling machines and nodes. Ansible is installed on the controlling computer, and the controlling machines manage the nodes via SSH. 

The controlling machine contains an inventory file that holds the node system’s location. Ansible runs the playbook on the controlling machine to deploy the modules on the node systems. Since Ansible is agentless, there’s no need for a third-party tool to connect the nodes.

4. “playbook” is.

A playbook has a series of YAML-based files that send commands to remote computers via scripts. Developers can configure entire complex environments by passing a script to the required systems rather than using individual commands to configure computers from the command line remotely. Playbooks are one of Ansible’s strongest selling points and often referred to as the tool’s building blocks.

5. What is “idempotency”?

idempotency is an important Ansible feature. It prevents unnecessary changes in the managed hosts. With idempotency, you can execute one or more tasks on a server as many times as you need to, but it won’t change anything that’s already been modified and is working correctly. To put it in basic terms, the only changes added are the ones needed and not already in place.

6. Ansible Galaxy?

This is a tool bundled with Ansible to create a base directory structure. Galaxy is a website that lets users find and share Ansible content. You can use this command to download roles from the website:

7. Ansible to create encrypted files?

To create an encrypted file, use the ‘ansible-vault create’ command.

$ ansible-vault create filename.yaml

You will get a prompt to create a password, and then to type it again for confirmation. You will now have access to a new file, where you can add and edit data.

8. What are “facts” in the context of Ansible?

Facts are newly discovered and known system variables, found in the playbooks, used mostly for implementing conditionals executions. Additionally, they gather ad-hoc system information.

9. Explain what an ask_pass module is.

It’s a playbook control module used to control a password prompt. It’s set to True by default.

10. What’s an ad hoc command?

Users initiate ad hoc commands to initiate actions on a host without using a playbook. Consider it a one-shot command.

11. Explain the difference between a playbook and a play.

A play is a set of tasks that run on one or more managed hosts. Plays consist of one or more tasks. A playbook consists of one or more plays.

Playbook:

- A playbook is a YAML file that defines a set of configurations, tasks, and plays to be executed by Ansible.
It can contain multiple plays, each targeting different groups of hosts or servers.
- Playbooks are the main mechanism used to orchestrate and automate tasks in Ansible.
- Playbooks can include variables, roles, tasks, handlers, and other directives to define the desired state of the infrastructure.

Play:

- A play is a section within a playbook that defines a set of tasks to be executed on a group of hosts.
- Each play targets a specific group of hosts defined in the inventory file.
- A play consists of a list of tasks, along with optional directives such as variables, handlers, and roles.
- Plays are executed sequentially, one after the other, in the order they appear in the playbook.

Task:

- A task is a single unit of work to be performed by Ansible.
- Tasks are defined within plays and represent individual steps to be executed on remote hosts.
- Each task typically performs a specific action, such as installing a package, copying a file, restarting a service, etc.
- Tasks are written in YAML format and can include modules, parameters, conditionals, loops, and other directives.

12. What exactly is a configuration management tool?

Configuration management tools help keep a system running within the desired parameters. They help reduce deployment time and substantially reduce the effort required to perform repetitive tasks. Popular configuration management tools on the market today include Chef, Puppet, Salt, and of course, Ansible.

13. What’s a handler?

In Ansible, a handler is similar to a regular task in a playbook, but it will only run if a task alerts the handler. Handlers are automatically loaded by roles/<role_name>/handlers/main.yaml. Handlers will run once, after all of the tasks are completed in a particular play.

14. Explain a few of the basic terminologies or concepts in Ansible

A few of the basic terms that are commonly used while operating on Ansible are as follows:

- Controller Machine: The controller machine is responsible for provisioning servers that are being managed. It is the machine where Ansible is installed.
- Inventory: An inventory is an initialization file that has details about the different servers that you are managing.
- Playbook: It is a code file written in the YAML format. A playbook basically contains the tasks that need to be executed or automated.
- Task: Each task represents a single procedure that needs to be executed, e.g., installing a library.
- Module: A module is a set of tasks that can be executed. Ansible has hundreds of built-in modules, but you can also create custom ones.
- Role: An Ansible role is a predefined way of organizing playbooks and other files to facilitate sharing and reusing portions of provisioning.
- Play: A task executed from start to finish or the execution of a playbook is called a play.
- Facts: Facts are global variables that store details about the system such as network interfaces or operating systems.
- Handlers: Handlers are used to trigger the status of a service such as restarting or stopping a service.

15. Where are tags used?

A tag is an attribute that sets the Ansible structure, plays, tasks, and roles. When an extensive playbook is needed, it is more useful to run just a part of it as opposed to the entire thing. That is where tags are used.

16. Which protocol does Ansible use to communicate with Linux and Windows?

In Linux systems, the Secure Shell (SSH) protocol is employed, while Windows systems utilize the Windows Remote Management (WinRM) protocol.

17. What are ad hoc commands? Give an example.

Ad hoc commands are simple, one-line commands used to perform a certain task. You can think of ad hoc commands as an alternative to writing playbooks. An example of an ad hoc command is as follows:
```Command: ansible host -m netscaler -a "nsc_host=nsc.example.com user=apiuser password=apipass"
```

18. Define Ansible inventory and its types.

An Ansible inventory file is used to define hosts and groups of hosts upon which the tasks, commands, and modules in a playbook will operate.

In Ansible, there are two types of inventory files, namely static and dynamic, which are explained below:

- Static Inventory: Static inventory file is a list of managed hosts declared under a host group using either hostnames or IP addresses in a plain text file. The managed host entries are listed below the group name in each line.
- Dynamic Inventory: Dynamic inventory is generated by a script written in Python, any other programming language, or, preferably, using plug-ins. In a cloud setup, static inventory file configuration will fail since IP addresses change once a virtual server is stopped and started again.

19. What is an Ansible vault?

Ansible vault is used to keep sensitive data, like passwords, rather than placing it as plain text in playbooks or roles. Any structured data file or single value inside a YAML file can be encrypted by Ansible.

20. How to generate encrypted passwords for a user module?

We can do this by using a small code, which is given below:

```
ansible all -i localhost, -m debug -a "msg={{ 'mypassword' | password_hash('sha512', 'mysecretsalt') }}"
```
We can also use the Passlib library in Python, which is mentioned below:


```
Command: python -c "from passlib.hash import sha512_crypt; import getpass; print(sha512_crypt.using(rounds=5000).hash(getpass.getpass()))"

```
21. Do you know how to turn off the facts in Ansible?

If you do not need any factual data about the hosts and you know everything about the systems centrally, then you may turn off fact gathering. This is advantageous when scaling Ansible in push mode with very large numbers of systems, mainly, or if we are using Ansible on experimental platforms. The following command can be used  to turn off the facts in Ansible:

```
Command:
- hosts: whatever
gather_facts: no
```



# JENKINS

1. What is Jenkins?

Jenkins is an open-source free automation tool used to build and test software projects. The tool makes it painless for developers to integrate changes to the project. Jenkins' primary focus is to keep track of the version control system and initiate and monitor a build system if there are any changes. It keeps an eye on the entire process and provides reports and notifications to alert.

Some typical reasons as to why Jenkins is so widely used are:

- Developers and testers use Jenkins to detect defects in the software development lifecycle and automate the testing of builds. 
- They use it to continuously monitor the code in real-time and integrate changes into the build.
- Jenkins as it turns out, is a great fit for building a CI/CD pipeline because of its plugin-capabilities, and simple-to-use nature.

2. What are the features of Jenkins?

Some of the crucial features of Jenkins are the following:

- It is a free and open-source automation tool
- Jenkins provides a vast number of plugins
- It is easy to set up and install on multiple operating systems
- Provides pipeline support
- Fast release cycles 
- Easy upgrades

4. How do you install Jenkins?

Follow the steps mentioned below to install Jenkins:

Install Java 
Install Apache Tomcat Server
Download Jenkins war File
Deploy Jenkins war File

5. What is "Continuous Integration" with reference to Jenkins?

Continuous Integration is a development practice where the codes can be integrated into a shared repository. 
The practice uses automated verifications for the early detection of code problems. 
Continuous Integration triggers the build to find and identify bugs present in the code.
It adds consistency to the build process.
It’s a means to build things faster and prevents broken code.

6. What is a Jenkins pipeline?

The pipeline represents the continuous delivery and continuous integration of all the jobs in the SDLC and DevOps life cycle. 
The Jenkins pipeline is a set of plugins that support implementation and integration of continuous delivery pipelines into Jenkins. It connects this pipeline in a particular format by Jenkins.
The Jenkins pipeline solves several problems like the maintenance of thousands of jobs and maintaining deployment with needing to resort to other powerful methods.

7.Name the three different types of pipelines in Jenkins?
The three different types of Jenkins pipelines are:

CI/CD pipeline 
Scripted pipeline
Declarative pipeline

8.  How can you create a backup and copy files in Jenkins?

Jenkins stores all the settings, builds scripts, and logs in the home directory. 
Then, if you want to create a backup of this Jenkins set up all you have to do is copy this directory. 
The job directory may also be copied to clone a job or rename the directory.

9. How can you set up a Jenkins job?

To set up a Jenkins job, you may follow these steps:

- Select New item from the menu
- Next, enter a name for the job and select a free-style job
- Click on OK to create a new job
- Hence, the next page that appears will allow you to configure your job.

10. What could be the steps to move or copy Jenkins from one server to another?

There are multiple ways to move or copy Jenkins from one server to another:

- You may move a job from one Jenkins installation to another just by copying the corresponding job directory.
- You may make a copy of an already existing job by making a clone of the job directory with an uncommon name.
- You may also just rename a current job by renaming a directory.

11. Assume that you have a pipeline. The first job that you performed was successful, but the second one failed.  What would you do now?

You don't have to worry, and you just have to restart the pipeline from the point where it failed by doing 'restart from stage.'

12. Explain the process in which Jenkins works?

Here’s the process in which Jenkins works:

- Jenkins checks changes in repositories regularly, and developers must secure their code regularly. 
- Once the changes are defined, Jenkins detects them and uses them to prepare a new build.
- After that, Jenkins will transverse through various stages in its usual pipeline. As one stage completes, the process will move further on to the next stage.
- If a stage fails, the Jenkins build will stop there, and the software will email the team using it. When completed successfully, the code implements itself in the proper server so that testing begins.
- After the successful testing phase, Jenkins shares the results with the team using it.

13. What is Jenkinsfile? 

Jenkins file is a text file that has a definition of a Jenkins pipeline and is checked into the source control repository. It enables code review and iteration on the pipeline. It also permits an audit trail for the pipeline.

14. What is the process to integrate Git with Jenkins?

To integrate Git with Jenkins, you can follow the following steps:

- First, create a new Jenkins job and open the Jenkins dashboard.
- Now, enter the desired project name and select the job type. 
- Click on OK.
- Then enter the project information. 
- After that, visit the 'Source Code Management' tab. 


Source: https://plugins.jenkins.io/git/

- If the Git plugin is pre-installed in Jenkins, there will be 'Git'.
- If it is not installed, you must reinstall the plugins (GitHub plugin, GitHub Branch Source plugin, GitHub API plugin, Git client plugin, etc.).
- After we install the plugins, restart Jenkins.
To check if Git is installed, you can go to Command Prompt and type Git, and you would see various options like usage, version, help, etc.

15. What is DSL Jenkins?

DSL stands for Domain Specific Language. Jenkins job DSL is a plugin that allows us to define jobs in the programmatic form with minimal effort. You can describe your jobs in Jenkins using a Groovy Based Language. They designed Jenkins job DSL plugin to create versions of the job, manage the records

16. What is the process to configure Third-party tools in Jenkins?

The process to configure Third-party tools in Jenkins can be seen in four significant steps:

- Install the third-party software
- Then install a Jenkins plugin supporting the third-party tool
- Now, configure the tool from the Manage Jenkins section
- Finally, your plugin is ready to be used

17. What is the process of making a Multibranch Pipeline in Jenkins?

To create a Multibranch Pipeline in Jenkins, follow the following steps:

- Open the Jenkins dashboard and create a new item by clicking on 'new item'
- Enter the project name and, from the options, select 'Multibranch pipeline'
- Click on OK

- Then select the repository location, branch source (GitHub/Bitbucket), and add the branch source credentials.
- Save the project
- Now, Jenkins automatically creates new Multibranch Pipelines for repositories
- Then to connect to the GitHub repo, we need the HookURL
- To get this URL from the repository settings, add this HookURL to the Webhooks section
- Once the jobs are created, Jenkins will automatically trigger the build

18. How can the parameters be defined in Jenkins?

In Jenkins, a build can take many input parameters to execute. 

- To define parameters for the job, select the “this project is parameterized” box.
- The drop down “Add Parameter” is enabled with the parameter types list. Any number of parameters may be added in the list.
There are several parameter types provided in the list. 

19. Explain the ways to configure Jenkins node agent to communicate with Jenkins master?

There are two ways to configure Jenkins node agent to communicate with Jenkins master:

- Browser–If we launch the Jenkins node agent from a browser, a Java Web Start or JNLP file is downloaded. The downloaded file launches a new process on the client machine to run jobs.
- Command-line–If you want to start the node agent using the command line, you need the executable agent.jar file. When this file runs, it launches a client's process to communicate with the Jenkins master to run build jobs.

20. What are the three security mechanisms Jenkins uses to authenticate users? 

The three mechanisms are as follows:

- Jenkins uses an internal database to store user data and credentials.
- Jenkins can use a lightweight Directory Access Protocol (LDAP) server to authenticate users.
- We can configure Jenkins to employ the application server's authentication mechanism upon which we deploy it.

21. Jenkins workflow and write a script for this workflow?

A Jenkins workflow is a sequence of steps or stages that define how a software project is built, tested, and deployed using Jenkins. A Jenkins workflow can be written using a Jenkins pipeline script, which is a DSL based on Groovy. A Jenkins pipeline script can be written in two ways: declarative or scripted.

Here is an example of a Jenkins workflow and a declarative pipeline script for it:

- The workflow consists of four stages: Build, Test, Deploy, and Notify
- The Build stage clones the GitHub repository of the project and builds a Docker image using the Dockerfile
- The Test stage runs the unit tests using the npm test
- The Deploy stage pushes the Docker image to Docker Hub and deploys it to a Kubernetes cluster using kubectl
- The Notify stage sends an email notification with the build status and the deployment URL

```
pipeline {
    agent any // This means that the pipeline will run on any available agent
    stages {
        stage('Build') {
            steps {
                git 'https://github.com/ajitfawade/node-todo-cicd.git' // This will clone the GitHub repository to the agent's workspace
                docker.build('ajitfawade/node-todo-cicd') // This will build a Docker image using Dockerfile
            }
        }
        stage('Test') {
            steps {
                sh 'npm install' // This will install the dependencies using npm
                sh 'npm test' // This will run the unit tests using npm
            }
        }
        stage('Deploy') {
            steps {
                script {
                    docker.withRegistry('https://registry.hub.docker.com', 'docker-hub-credentials') { // This will use the credentials for Docker Hub that you need to create in Jenkins
                        docker.image('ajitfawade/node-todo-cicd').push() // This will push the Docker image to Docker Hub
                    }
                    withCredentials([usernamePassword(credentialsId: 'kubernetes-credentials', usernameVariable: 'KUBE_USER', passwordVariable: 'KUBE_PASS')]) { // This will use the credentials for Kubernetes that you need to create in Jenkins
                        sh "kubectl --username=${KUBE_USER} --password=${KUBE_PASS} apply -f k8s.yaml" // This will deploy the Docker image to Kubernetes using kubectl and k8s.yaml file
                    }
                }
            }
        }
        stage('Notify') {
            steps {
                emailext ( // This will send an email notification using Email Extension Plugin that you need to install in Jenkins
                    subject: "${env.JOB_NAME} - Build # ${env.BUILD_NUMBER} - ${currentBuild.currentResult}",
                    body: """<p>${env.JOB_NAME} - Build # ${env.BUILD_NUMBER} - ${currentBuild.currentResult}</p>
                             <p>Check console output at <a href="${env.BUILD_URL}">${env.BUILD_URL}</a></p>
                             <p>Access deployed application at <a href="http://node-todo-cicd.k8s.io">http://node-todo-cicd.k8s.io</a></p>""",
                    to: 'ajitfawade@gmail.com'
                )
            }
        }
    }
}
```
How to create a continuous deployment in Jenkins?

To create a continuous deployment in Jenkins, you need to do the following:

- Create a pipeline job that defines and executes the steps or stages for building, testing, and deploying your software project using a Jenkins pipeline script, which is a DSL based on Groovy. You can write the pipeline script in two ways: declarative or scripted. A declarative pipeline is a more structured and simplified way of writing a pipeline using predefined syntax and keywords. A scripted pipeline is a more flexible and expressive way of writing a pipeline using Groovy code.
- Configure the build triggers for the pipeline job to run automatically whenever there is a code change in the source repository or whenever there is a manual trigger from the user. You can use various options such as SCM polling, webhook, cron expression, etc., to set up the build triggers.
- Configure the deployment stage of the pipeline job to deploy the software to the target environment using the appropriate tools and commands. You can use various plugins or tools such as Docker, Kubernetes, AWS, Azure, etc., to perform the deployment.
- Configure the post-build actions or steps for the pipeline job to notify the stakeholders about the deployment status and outcome. You can use various plugins or tools such as Email Extension, Slack, Teams, etc., to send notifications.

How to build a job in Jenkins?

To build a job in Jenkins, you can use one of the following methods:

- Manual build: You can manually trigger a build by clicking on the Build Now button on the job page or by using the Build with Parameters option if the job has parameters.
- Scheduled build: You can schedule a build to run at a specific time or interval by using the Build periodically option in the build triggers section of the job configuration. You can use a cron expression to specify the schedule.
- Triggered build: You can trigger a build by an external event or another job by using the options such as GitHub hook trigger for GITScm polling, Poll SCM, Build after other projects are built, etc., in the build triggers section of the job configuration. You can also use the Remote access API to trigger a build programmatically.

2. Why do we use a pipeline in Jenkins?

We use pipeline in Jenkins because it provides several advantages such as:

- It allows us to define and execute a series of steps or stages as a single continuous process using a DSL based on Groovy.
- It enables us to write our pipeline code in a version-controlled repository along with our source code, which improves collaboration and maintainability.
- It supports parallel execution, error handling, shared libraries, etc., which provide more control and flexibility over our pipeline logic.
- It offers a graphical representation of our pipeline stages and their status, which improves visibility and troubleshooting

23. How will you handle secrets?

To handle secrets in Jenkins, we can use one of the following methods:

- Credentials plugin: This is a built-in plugin that allows us to store and manage secrets such as username and password, SSH key, API token, etc., in Jenkins. We can create credentials in Jenkins either globally or per project and use them in our jobs or pipelines. We can also use a credentials binding plugin to inject credentials as environment variables in our jobs or pipelines.
- Secret text plugin: This is an extension of the credentials plugin that allows us to store and manage secrets as plain text in Jenkins. We can create secret text credentials in Jenkins either globally or per project and use them in our jobs or pipelines. We can also use a secret text binding plugin to inject secret text as environment variables in our jobs or pipelines.
- HashiCorp Vault plugin: This is an external plugin that allows us to integrate Jenkins with HashiCorp Vault, which is a tool for securely storing and accessing secrets. We can configure Vault credentials in Jenkins either globally or per project and use them in our jobs or pipelines. We can also use the Vault binding plugin to inject Vault secrets as environment variables in our jobs or pipelines.

24. Name some of the plugins in Jenkins?

Some of the plugins in Jenkins are:

- Git plugin: This plugin allows us to integrate Jenkins with Git, which is a distributed version control system. It enables us to clone, fetch, checkout, merge, and push Git repositories in our jobs or pipelines.
- Maven plugin: This plugin allows us to integrate Jenkins with Maven, which is a build automation tool for Java projects. It enables us to invoke Maven goals and phases in our jobs or pipelines.
- Docker plugin: This plugin allows us to integrate Jenkins with Docker, which is a tool for building and running containerized applications. It enables us to build, run, push, and pull Docker images and containers in our jobs or pipelines.
- Kubernetes plugin: This plugin allows us to integrate Jenkins with Kubernetes, which is a platform for managing containerized workloads and services. It enables us to run dynamic agents on Kubernetes pods and deploy applications to Kubernetes clusters in our jobs or pipelines.
- Email Extension plugin: This plugin allows us to enhance the email notification functionality of Jenkins. It enables us to send customized email notifications with rich content and attachments in our jobs or pipelines.

25. How To Trigger a Build In Jenkins Manually?

To manually trigger a build in Jenkins:

- Access the Jenkins Dashboard.
- Select the specific Jenkins job.
- Click “Build Now” to start the manual build.
- Provide build parameters if necessary.
- Confirm and monitor the build progress in real time.
- Review the build results on the job’s dashboard.
- Access build artifacts if applicable.
- Trigger additional builds as needed.

26. How To Integrate Git With Jenkins?

To integrate Git with Jenkins:

- Install the “Git Plugin” in Jenkins through the plugin manager.
- Configure Git in the global tool configuration, ensuring automatic installation is enabled.
- Create or configure a Jenkins job, selecting Git as the version control system.
- Specify the Git repository URL and, if necessary, credentials for authentication.
- Define the branches to monitor and build.
- Set up build triggers as needed.
- Save the job configuration and trigger builds manually or automatically based on your settings.
- Monitor build progress and results in the Jenkins dashboard.

27. What Does “Poll SCM” Mean In Jenkins?

In Jenkins, “poll SCM” means periodically checking a version control system (e.g., Git) for changes. You can schedule how often Jenkins checks for updates. When changes are detected, Jenkins triggers a build, making it a key feature for continuous integration, scheduled tasks, and automated response to code changes.

28. How To Schedule Jenkins Build Periodically (hourly, daily, weekly)? Explain the Jenkins schedule format.

To schedule Jenkins builds periodically at specific intervals, you can use the built-in scheduling feature. Jenkins uses a cron-like syntax for scheduling, allowing you to specify when and how often your builds should run. Here’s a detailed explanation of the Jenkins schedule format and how to schedule builds:

1. Jenkins Schedule Format
The Jenkins schedule format closely resembles the familiar cron syntax, with a few minor differences. A typical Jenkins schedule consists of five fields, representing minute, hour, day of the month, month, and day of the week, in that order:

Here’s what each field means:

Minute (0 – 59): Specifies the minute of the hour when the build should run (e.g., 0 for the top of the hour, 30 for the half-hour).
Hour (0 – 23): Specifies the hour of the day when the build should run (e.g., 1 for 1 AM, 13 for 1 PM).
Day of the month (1 – 31): Specifies the day of the month when the build should run (e.g., 1 for the 1st day of the month, 15 for the 15th day).
Month (1 – 12): Specifies the month when the build should run (e.g., 1 for January, 12 for December).
Day of the week (0 – 7): Specifies the day of the week when the build should run (e.g., 0 or 7 for Sunday, 1 for Monday, and so on).

Configuring The Schedule In Jenkins
To schedule a build in Jenkins:

Open your Jenkins job’s configuration page.
In the “Build Triggers” section, check the “Build periodically” option.
In the text box that appears, enter your desired schedule using the cron-like syntax.
For example, to schedule a daily build at midnight (00:00), enter 0 0 * * *. Make sure to include the five fields in the schedule.

Click “Save” to apply the schedule.
Jenkins will now automatically trigger your builds according to the specified schedule. You can use this scheduling feature to automate tasks, such as nightly builds, daily backups, or any other recurring job that fits your project’s needs.

29. What Is Jenkins Home Directory Path?

The Jenkins home directory is where Jenkins stores its critical data, including job configurations, logs, plugins, and more. The location of this directory varies by operating system but can typically be found at:

Linux/Unix: /var/lib/jenkins
Windows: C:\Users<YourUsername>.jenkins
macOS: /Users/<YourUsername>/.jenkins
You can configure its location during installation or in the Jenkins startup script. Understanding this directory is essential for managing and backing up Jenkins data.

30. How To Integrate Slack With Jenkins?

To integrate Slack with Jenkins for notifications:

- Set up a Slack Incoming Webhook in your Slack workspace to get a Webhook URL
- Install the “Slack Notification” plugin in Jenkins.
- Configure Jenkins global Slack settings by adding the Slack Webhook URL.
In your Jenkins job configuration, add a “Slack Notifications” post-build action.
Specify the Slack channel, customize message options, and select notification preferences (e.g., success, failure).
Save the job configuration.
Run a build, and Jenkins will send notifications to the specified Slack channel based on build results.
Now, Jenkins is integrated with Slack, providing real-time notifications to keep your team informed about build status and progress.


32.  What Is A Jenkins Agent?

A Jenkins agent, also called a Jenkins slave or node, is a separate machine or resource that collaborates with a Jenkins master to execute jobs and build tasks. Agents enable parallel and distributed builds, scaling Jenkins’ capacity.

They register with the master, get assigned jobs, execute them on their own hardware or VMs, and report back results. Agents can run on various platforms, making it possible to test and build in different environments.

33. Types of build triggers in Jenkins.

Types of build triggers in Jenkins include:

- SCM Polling Trigger: Monitors source code repositories for changes and triggers builds.
- Scheduled Build Trigger: Runs jobs on a predefined schedule using cron-like syntax.
- Webhook Trigger: Listens for external events or notifications to start builds.
- Upstream/Downstream Trigger: Triggers downstream jobs based on the success of upstream jobs, creating build pipelines.
- Manual Build Trigger: Requires manual user intervention to start a job.
- Dependency Build Trigger: Triggers jobs when another job is completed, regardless of success or failure.
- Parameterized Trigger: Passes parameters from one job to another during triggering.
- Pipeline Trigger: Allows custom triggering logic within Jenkins Pipelines.

34. Explain about Master-Slave Configuration in Jenkins.

A Master-Slave configuration in Jenkins, also known as a Jenkins Master-Agent configuration, is a setup that allows Jenkins to distribute and manage its workload across multiple machines or nodes. In this configuration, there is a central Jenkins Master server, and multiple Jenkins Agent nodes (slaves) that are responsible for executing build jobs. This architecture offers several advantages, including scalability, parallelism, and the ability to run jobs in diverse environments.

Here’s an explanation of the key components and benefits of a Master-Slave configuration in Jenkins:

Components:
Jenkins Master:
The Jenkins Master is the central server responsible for managing and coordinating the entire Jenkins environment.
It hosts the Jenkins web interface and handles the scheduling of build jobs, job configuration, and the storage of build logs and job history.
The Master communicates with Jenkins Agents to delegate job execution and collects the results.
Jenkins Agent (Slave)
Jenkins Agents, often referred to as Jenkins Slaves or nodes, are remote machines or virtual instances that perform the actual build and testing tasks.
Agents can run on various operating systems and environments, enabling the execution of jobs in different configurations.
Agents are registered with the Jenkins Master and are available to accept job assignments.
Benefits:
Scalability: Easily handle more build jobs by adding Agents.
Parallelism: Run multiple jobs simultaneously for faster results.
Resource isolation: Isolate jobs on different machines or environments.
Load distribution: Distribute jobs for optimal resource use.
Flexibility: Configure Agents for specific requirements.
Resilience: Reassign jobs if an Agent becomes unavailable.
Security and isolation: Control Agent access and resources.
Support for diverse environments: Test on various platforms and setups.
This architecture streamlines CI/CD pipelines and enhances resource utilization.

35. How to maintain a CI/CD pipeline of Jenkins in GitHub?

To maintain a CI/CD pipeline in Jenkins with GitHub, follow these steps:

Version control Jenkins configuration using Git.
Define the pipeline with a Jenkinsfile in the project’s GitHub repository.
Set up webhooks in GitHub to trigger Jenkins pipelines.
Manage sensitive data securely with Jenkins credentials.
Keep Jenkins plugins up to date for the latest features and security.
Regularly review and update pipeline configurations.
Include automated tests for pipeline configuration.
Monitor build logs for issues and failures.
Use version control for pipeline code to enable rollbacks.
Consider Infrastructure as Code (IaC) for infrastructure provisioning.
Maintain documentation for the CI/CD pipeline.
Encourage collaboration and code reviews for pipeline improvements.
Implement backups and disaster recovery plans.
Ensure compliance and security in your CI/CD pipeline.
These steps will help you keep your Jenkins CI/CD pipeline up-to-date and reliable while integrating with your GitHub repository.

36. How would you design and implement a Continuous Integration and Continuous Deployment (CI/CD) pipeline for deploying applications to Kubernetes?

Designing and implementing a CI/CD pipeline for deploying applications to Kubernetes involves several key steps and considerations to ensure a smooth and automated deployment process. Below is a high-level guide on how to design and implement such a pipeline:

Step 1: Set Up a Version Control System (VCS)
Use a version control system like Git to manage your application code and deployment configurations. Host your Git repository on a platform like GitHub or GitLab.
Step 2: Define Kubernetes Manifests
Create Kubernetes manifests (YAML files) to describe your application’s deployment, services, ingress controllers, and other resources. Store these manifests in your Git repository.
Step 3: Choose a CI/CD Tool
Select a CI/CD tool that integrates well with Kubernetes and your VCS. Popular choices include Jenkins, GitLab CI/CD, Travis CI, CircleCI, and others.
Step 4: Configure CI/CD Pipeline
Define a CI/CD pipeline configuration file (e.g., .gitlab-ci.yml or Jenkinsfile) in your Git repository. This file specifies the stages and steps of your pipeline.
Configure the pipeline to trigger code pushes to the VCS, merge requests, or other relevant events.
Step 5: Build and Test Stage
In the initial stage of the pipeline, build your application container image. Use Docker or another containerization tool.
Run tests against your application code to ensure its correctness. This stage may include unit tests, integration tests, and code quality checks.
Step 6: Container Registry
Push the built container image to a container registry like Docker Hub, Google Container Registry, or an internal registry.
Ensure that your pipeline securely manages registry credentials.
Step 7: Deployment Stage
Deploy your application to Kubernetes clusters. This stage involves applying Kubernetes manifests to create or update resources.
Use tools like kubectl or Kubernetes-native deployment tools like Helm to manage deployments.
Implement a rolling update strategy to minimize downtime during deployments.
Step 8: Testing Stage
After deploying to Kubernetes, perform additional tests, including end-to-end tests and smoke tests, to verify that the application runs correctly in the cluster.
Step 9: Promotion to Production
Implement a promotion strategy to move successfully tested changes from staging to production environments. This can involve manual approval gates or automated processes.
Step 10: Monitoring and Logging
Integrate monitoring and logging tools (e.g., Prometheus, Grafana, ELK stack) to track the health and performance of your applications in the Kubernetes cluster. – Implement alerting to notify teams of issues that require attention.
Step 11: Security and Access Control
Implement security measures, including RBAC (Role-Based Access Control) and Pod Security Policies, to ensure that only authorized users and applications can access your cluster.
Step 12: Infrastructure as Code (IaC)
Treat your Kubernetes cluster’s infrastructure as code using tools like Terraform or Kubernetes operators. This ensures that your cluster infrastructure is versioned and can be recreated as needed.
Step 13: Documentation and Training
Document your CI/CD pipeline processes, including setup, configurations, and troubleshooting steps. Provide training to team members on pipeline usage and best practices.
Step 14: Continuous Improvement
Continuously monitor and evaluate the effectiveness of your CI/CD pipeline. Seek feedback from the development and operations teams to identify areas for improvement. – Make incremental updates and optimizations to enhance the pipeline’s efficiency and reliability.
Step 15: Security Scans and Compliance
Integrate security scanning tools into your pipeline to identify and address vulnerabilities in your application code and container images. – Ensure compliance with industry-specific regulations and security standards.
By following these steps and best practices, you can design and implement a robust CI/CD pipeline for deploying applications to Kubernetes. This pipeline automates the deployment process, ensures consistency, and enables rapid and reliable application delivery in a Kubernetes environment.

37. Explain about the multibranch pipeline in Jenkins.

A Multibranch Pipeline in Jenkins is a feature for managing CI/CD pipelines for multiple branches in a version control repository. It automatically creates pipelines for each branch or pull request, uses Jenkinsfiles to define pipeline configurations, supports parallel builds, and cleans up unused jobs. It simplifies managing and automating pipelines across various code branches and pull requests, streamlining the CI/CD process.

38. What is a Freestyle project in Jenkins?

A Freestyle project in Jenkins is a basic and user-friendly job type. It allows users to configure build jobs using a graphical interface without scripting. It’s suitable for simple build and automation tasks, supporting various build steps, post-build actions, and integration with plugins. While it’s easy to use, it may not be ideal for complex workflows, unlike Jenkins Pipeline jobs, which offer more flexibility and scripting capabilities.

20. What is a Multi-Configuration project in Jenkins?

A Multi-Configuration project in Jenkins, also known as a Matrix Project, is designed for testing or building a software project across multiple configurations simultaneously. It allows you to define axes representing different variations (e.g., operating systems, JDK versions) and Jenkins automatically tests or builds the project for all possible combinations of these configurations. It’s useful for cross-platform testing, version compatibility, browser testing, localization checks, and more, ensuring software works in diverse environments.

21. What is a Pipeline in Jenkins?

A Jenkins Pipeline is a series of code-defined steps that automate the Continuous Integration and Continuous Delivery (CI/CD) process. It allows you to define and manage your entire software delivery pipeline as code, using a declarative or scripted syntax. Pipelines cover continuous integration, delivery, and deployment, with support for parallel and sequential stages. They integrate with source control, allow customization, utilize build agents, and offer extensive plugin support. This approach promotes automation, collaboration, and repeatability, making software development and delivery more efficient and reliable.

22. How to mention the tools configured in the Jenkins pipeline?

In a Jenkins pipeline, you can mention the tools and configurations used by defining them in the pipeline script itself. This is typically done in the ‘tools’ section of your pipeline script. Below are the steps to mention and configure tools in a Jenkins pipeline:

Step1: Open or Create a Jenkinsfile
Ensure that you have a Jenkinsfile in your project repository. If you don’t have one, create a new file named Jenkinsfile in the root directory of your project.

Step 2: Define Pipeline and Tools Section
In the Jenkinsfile, define your pipeline using the pipeline block, and within that block, define a tools section. The tools section is used to specify which tools or tool installations should be available for the pipeline.

```
pipeline {
    agent any
    tools {
        // Define the tools and their configurations here
        // Example:
        maven 'MavenTool' // Name of the tool and the tool installation name
        jdk 'JDKTool'    // Name of the tool and the tool installation name
    }
    stages {
        // Define your pipeline stages here
        stage('Build') {
            steps {
                // Use the configured tools in your pipeline stages
                // Example:
                script {
                    sh '''#!/bin/bash
                    echo "Building with Maven"
                    mvn clean package
                    '''
                }
            }
        }
    }
}
```

Step 3: Specify Tool Installations
In the tools section, specify the tools you want to use along with their installation names. The installation names should match the names configured in your Jenkins master’s tool configurations. For example, if you have defined a Maven installation named “MavenTool” and a JDK installation named “JDKTool” in Jenkins, you can reference them in your pipeline as shown above.

Step 4: Use the Configured Tools
In your pipeline stages, you can now use the configured tools. For example, if you specified a Maven tool, you can use it to build your project by invoking mvn with the configured Maven installation

```
stage('Build') {
    steps {
        sh '''#!/bin/bash
        echo "Building with Naveen"
        mvn clean package
        '''
    }
}
```

Step 5: Save and Commit
Save the Jenkinsfile and commit it to your version control system (e.g., Git). This ensures that your pipeline configuration is versioned and can be shared with your team.

Step 6: Run the Pipeline
Trigger the Jenkins pipeline, and it will automatically use the tools and configurations you specified to build, test, and deploy your project.

By following these steps and configuring tools within your Jenkins pipeline script, you ensure that your pipeline has access to the required tools and environments, making your builds and deployments consistent and reproducible.

23. What is the global tool configuration in Jenkins?

Global Tool Configuration in Jenkins refers to the centralized configuration of software tools and installations that can be used by all Jenkins jobs and pipelines across the Jenkins master server. It allows Jenkins administrators to set up and manage tool installations such as JDKs, build tools (e.g., Maven, Gradle), version control systems (e.g., Git, Subversion), and other utilities in a consistent and organized manner. This configuration is accessible from the Jenkins web interface and provides a convenient way to ensure that all Jenkins projects have access to the required tools.

24. Write a sample Jenkins pipeline example.

Here’s a simple Jenkins pipeline example written in Declarative Pipeline syntax. This example demonstrates a basic pipeline that checks out code from a Git repository, builds a Java project using Maven, and then archives the build artifacts:

```
pipeline {
    agent any

    stages {
        stage('Checkout') {
            steps {
                checkout([$class: 'GitSCM', branches: [[name: '*/main']], 
                userRemoteConfigs: [[url: 'https://github.com/your/repository.git']]])
            }
        }

        stage('Build') {
            steps {
                sh 'mvn clean package'
            }
        }

        stage('Archive Artifacts') {
            steps {
                archiveArtifacts artifacts: 'target/*.jar', allowEmptyArchive: true
            }
        }
    }

    post {
        success {
            echo 'Pipeline completed successfully'
        }
        failure {
            echo 'Pipeline failed'
        }
    }
}
```

In this pipeline
The pipeline is defined using the pipeline block.
It runs on any available agent (specified by agent any), meaning it can be executed on any available Jenkins agent or node.
The pipeline has three stages: Checkout, Build, and Archive Artifacts.
In the Checkout stage, the code is checked out from a Git repository using the checkout scm step. Replace ‘your-git-repo-url’ with the actual URL of your Git repository.
In the Build stage, the maven tool is used to build a Java project. The sh ‘mvn clean package’ command executes the Maven build.
The Archive Artifacts stage archives the built artifacts (JAR files in this example) using the archived artifacts step. The target/*.jar pattern should be adjusted to match the location of your project’s output.
The post section defines post-build actions. In this example, it includes simple echo statements, but you can customize this section to trigger notifications or perform additional actions based on the build result (success or failure).
This is a basic Jenkins pipeline example, but Jenkins pipelines can be much more complex and versatile, depending on your project’s needs. You can extend and customize pipelines to include additional stages, steps, and integrations with other tools and services as required for your CI/CD process.

40. If There Is a Broken Build In a Jenkins Project, What Steps Would You Take To Troubleshoot And Resolve The Issue?

To troubleshoot and resolve a broken build in Jenkins:

Identify the failure by examining the console output for error messages and clues.
Review recent code changes to see if commits may have introduced issues.
Verify dependencies and the build environment.
Check the Jenkins job configuration for accuracy.
Investigate failed tests to pinpoint code issues.
Examine log and artifact files for additional information.
Debug the code if necessary.
Revert or isolate changes to identify the problematic code.
Collaborate with the team to gather insights.
Implement fixes by correcting code, updating dependencies, or adjusting configurations.
Test fixes locally before committing them.
Monitor future builds to ensure the issue is resolved.
These steps will help maintain a reliable CI pipeline.

33. What Are The Different Types Of Jenkins Jobs?

Jenkins offers a variety of job types to accommodate different automation and build needs. Some common types include:

- Freestyle Project: Basic job with a simple UI for build steps.
- Pipeline Project: Define build processes as code using Groovy scripts.
- Multi-configuration Project: Build and test on multiple configurations in parallel.
- GitHub Organization Project: Automate CI/CD for GitHub repositories.
- Maven Project: Specifically for Java projects using Maven.
- Folder: Organize and group related jobs.
- External Job: Trigger builds on remote Jenkins instances.
- GitHub PR Builder: Automate PR builds in GitHub repositories.
- Copy Artifact Project: Copy build artifacts between jobs.
- Parameterized Build: Pass parameters to customize job execution.
- Build Flow: Orchestrate complex build processes with Groovy.
- GitHub Organization Folder: Organize GitHub repos within an organization.
- Freestyle with Maven: Blend freestyle and Maven build steps.
These job types suit various development and automation scenarios, providing flexibility and automation based on project needs. The choice depends on project requirements and workflow.

45. What is Jenkins Shared Library?

A Jenkins Shared Library is a powerful feature in Jenkins that allows organizations to centralize and reuse code, scripts, and custom functions across multiple Jenkins pipelines and jobs. It enables the creation of a shared and maintainable codebase that can be leveraged by various projects and teams, promoting consistency, efficiency, and code reuse in your Jenkins CI/CD workflows.

Key characteristics and aspects of Jenkins Shared Libraries include:

Reusable Code Components: Shared Libraries allow you to define common code components, such as custom steps, functions, and utilities, in a centralized location. These components can be written in Groovy (the scripting language used for Jenkins pipelines) and then reused across different Jenkins pipelines and jobs.
Modularization: Shared Libraries support the modularization of code, making it easier to manage and maintain. You can organize your code into multiple classes, methods, or files within the library, promoting clean and organized code architecture.
Custom Steps: You can create custom pipeline steps that encapsulate complex logic or repetitive tasks. These custom steps become available for use in any Jenkins pipeline that references the Shared Library.
Version Control: Shared Libraries are typically versioned and managed in a version control system (e.g., Git). This enables version control, code reviews, and collaborative development practices for your shared codebase.
Secure and Controlled Access: Access to Shared Libraries can be controlled through Jenkins security settings. You can restrict who can modify or contribute to the library while allowing other teams or users to consume the library in their pipelines.
Library Configuration: Shared Libraries can be configured at the Jenkins master level, making them accessible to all pipelines running on that Jenkins instance. Alternatively, you can configure libraries at the folder or pipeline level for more granular control.
Pipeline DSL Extensions: You can extend the Jenkins pipeline DSL (Domain Specific Language) by defining custom DSL methods within the Shared Library. These extensions can be used to simplify and streamline pipeline definitions.

45. What is RBAC, and how do you configure RBAC in Jenkins?

RBAC, or Role-Based Access Control, is a security model used in Jenkins to manage user permissions. To configure RBAC in Jenkins:

Install the “Role-Based Authorization Strategy” plugin.
Enable security and select “Role-Based Strategy” in the global security settings.
Create and manage roles representing job functions.
Assign roles to users or groups.
Save the configuration to enforce access control based on assigned roles.
RBAC ensures users have the appropriate access permissions in Jenkins, enhancing security and access control. Administrators typically retain an “Admin” role with full access. Permissions from multiple assigned roles are combined for user access.

https://www.geeksforgeeks.org/jenkins-interview-questions/

# Github actions

# Azure devops

# PYTHON
Are Arguments in Python Passed by Value or by Reference?
Arguments are passed in python by a reference. This means that any changes made within a function are reflected in the original object.

How is Memory managed in Python?
Python has a private heap space that stores all the objects. The Python memory manager regulates various aspects of this heap, such as sharing, caching, segmentation, and allocation. The user has no control over the heap; only the Python interpreter has access.

How Is Multithreading Achieved in Python?
Multithreading usually implies that multiple threads are executed concurrently. The Python Global Interpreter Lock doesn't allow more than one thread to hold the Python interpreter at that particular point of time. So multithreading in python is achieved through context switching. It is quite different from multiprocessing which actually opens up multiple processes across multiple threads.

How Would You Generate Random Numbers in Python?
To generate random numbers in Python, you must first import the random module. 

The random() function generates a random float value between 0 & 1.

> random.random()

The randrange() function generates a random number within a given range.

Syntax: randrange(beginning, end, step)

Example - > random.randrange(1,10,2)

What Does the // Operator Do?
In Python, the / operator performs division and returns the quotient in the float.

For example: 5 / 2 returns 2.5

The // operator, on the other hand, returns the quotient in integer.

For example: 5 // 2 returns 2

What Does the ‘is’ Operator Do?
The ‘is’ operator compares the id of the two objects. 

list1=[1,2,3]

list2=[1,2,3]

list3=list1

list1 == list2 🡪 True

list1 is list2 🡪 False

list1 is list3 🡪 True

What Is the Purpose of the Pass Statement?
The pass statement is used when there's a syntactic but not an operational requirement

What Is the Difference Between a List and a Tuple?
Lists are mutable while tuples are immutable.


What Is Docstring in Python?
This is one of the most frequently asked Python interview questions

Docstrings are used in providing documentation to various Python modules, classes, functions, and methods. 

Example - 
```
def add(a,b):

    '''This function adds two numbers.'''

    sum=a+b

    return sum

sum=add(10,20)

print("Accessing doctstring method 1:",add.__doc__)

print("Accessing doctstring method 2:",end="")

help(add)
```

How Do You Use Print() Without the Newline?

Python v3

>>print(“Hi”,end=“ ”)

>>print(“How are you?”)

Output: Hi. How are you?

How Do You Use the Split() Function in Python?
The split() function splits a string into a number of strings based on a specific delimiter. 

Syntax - 

string.split(delimiter, max)

Where:

the delimiter is the character based on which the string is split. By default it is space. 

max is the maximum number of splits 

Example - 

>>var=“Red,Blue,Green,Orange”

>>lst=var.split(“,”,2)

>>print(lst)

Output:

[‘Red’,’Blue’,’Green, Orange’]

Here, we have a variable var whose values are to be split with commas. Note that ‘2’ indicates that only the first two values will be split.

What Are *args and *kwargs?
*args 

It is used in a function prototype to accept a varying number of arguments.
It's an iterable object. 
Usage - def fun(*args)
*kwargs 

It is used in a function prototype to accept the varying number of keyworded arguments.
It's an iterable object
Usage - def fun(**kwargs):
fun(colour=”red”.units=2)

27. “in Python, Functions Are First-class Objects.” What Do You Infer from This?
It means that a function can be treated just like an object. You can assign them to variables, or pass them as arguments to other functions. You can even return them from other functions.

28. What Is the Output Of: Print(__name__)? Justify Your Answer.
__name__ is a special variable that holds the name of the current module. Program execution starts from main or code with 0 indentations. Thus, __name__ has a value __main__ in the above case. If the file is imported from another module, __name__ holds the name of this module.

Explain how Python is an interpreted language.
Any programming language that is not in machine-level code before runtime is called an interpreted language. Python is thus an interpreted language.

What is PEP 8?
PEP denotes Python Enhancement Proposal. It's a collection of guidelines for formatting Python code for maximum readability.

56. What are decorators in Python?
Decorators are used for changing the appearance of a function without changing its structure. Decorators are typically defined prior to the function they are enhancing.

57. How to use decorators in Python?
Decorators are typically defined prior to the function they are enhancing. To use a decorator, we must first specify its function. Then we write the function to which it is applied, simply placing the decorator function above the function to which it must be applied.

Differentiate between .pyc and .py.
The .py files are the source code files for Python. The bytecode of the python files are stored in .pyc files, which are created when code is imported from another source. The interpreter saves time by converting the source .py files to .pyc files.

Explain global variables and local variables in Python.
Local Variables:

A local variable is any variable declared within a function. This variable exists only in local space, not in global space.

Global Variables:

Global variables are variables declared outside of a function or in a global space. Any function in the program can access these variables.

70. On Unix, how do you make a Python script executable?
Script file should start with #!/usr/bin/env python.

75. What is _init_?
_init_ is a constructor or method in Python. This method is used to allocate memory when a new object is created.

76. What is the Lambda function?
A lambda function is a type of anonymous function. This function can take as many parameters as you want, but just one statement.

77. Why Lambda is used in Python?
Lambda is typically utilized in instances where an anonymous function is required for a short period of time. Lambda functions can be applied in two different ways:

Assigning Lambda functions to a variable
Wrapping Lambda function into another function

How does continue, break, and pass work?
Continue

When a specified condition is met, the control is moved to the beginning of the loop, allowing some parts of the loop to be transferred.

Break

When a condition is met, the loop is terminated and control is passed to the next statement.

Pass

When you need a piece of code syntactically but don't want to execute it, use this. This is a null operation. 

82. What are generators in Python?
Functions which return an iterable set of items are known as generators.

The assignment statement (= operator) in Python does not copy objects. Instead, it establishes a connection between the existing object and the name of the target variable. The copy module is used to make copies of an object in Python. Furthermore, the copy module provides two options for producing copies of a given object –

Deep Copy: Deep Copy recursively replicates all values from source to destination object, including the objects referenced by the source object.

from copy import copy, deepcopy

list_1 = [1, 2, [3, 5], 4]

## shallow copy

list_2 = copy(list_1) 

list_2[3] = 7

list_2[2].append(6)

list_2    # output => [1, 2, [3, 5, 6], 7]

list_1    # output => [1, 2, [3, 5, 6], 4]

## deep copy

list_3 = deepcopy(list_1)

list_3[3] = 8

list_3[2].append(7)

list_3    # output => [1, 2, [3, 5, 6, 7], 8]

list_1    # output => [1, 2, [3, 5, 6], 4]

Shallow Copy: A bit-wise copy of an object is called a shallow copy. The values in the copied object are identical to those in the original object. If one of the values is a reference to another object, only its reference addresses are copied.

 In Python, are arguments provided by value or reference?
Pass by value: The actual item's copy is passed. Changing the value of the object's copy has no effect on the original object's value.

Pass by reference: The actual object is passed as a reference. The value of the old object will change if the value of the new object is changed.

What is inheritance in Python?
Inheritance allows one class to gain all of another class's members (for example, attributes and methods). Inheritance allows for code reuse, making it easier to develop and maintain applications

What are the different types of inheritance in Python?
The following are the various types of inheritance in Python:

Single inheritance: The members of a single super class are acquired by a derived class.
Multiple inheritance: More than one base class is inherited by a derived class.
Muti-level inheritance: D1 is a derived class inherited from base1 while D2 is inherited from base2.
Hierarchical Inheritance: You can inherit any number of child classes from a single base class.

117. Explain polymorphism in Python.
The ability to take various forms is known as polymorphism. For example, if the parent class has a method named ABC, the child class can likewise have a method named ABC with its own parameters and variables. Python makes polymorphism possible.

118. What is encapsulation in Python?
Encapsulation refers to the joining of code and data. Encapsulation is demonstrated through a Python class.

119. In Python, how do you abstract data?
Only the necessary details are provided, while the implementation is hidden from view. Interfaces and abstract classes can be used to do this in Python.



# Bash

What is the alternative command for echo?

The alternative command for echo is tput. This command allows us to control how the output is displayed on the screen. 

Tell us about the ‘$#’ use in shell scripting.

'$#' is used to display the total number of passed arguments to the script. 

Name standard streams in Linux.

The standard streams in Linux are Standard Input, Standard Output, and Standard Error.

Explain Crontab.

Crontab means cron table because the tasks are executed using the job scheduler ‘cron.' It is a list of commands that run on a regular schedule, and the name of the command also manages the list. Crontab is the schedule and also the name of the program used to edit the schedule. 

6. Differentiate between $@ and $*.

$* considers an entire set of positional arguments as a single string whereas, $@ treats each quote argument as a separate argument.  

Tell us how you can compare strings in a shell script.

To compare the text strings, we use the ‘test’ command. This command compares text strings by comparing each character of each string. 

```
#!/bin/bash

# Define two strings
string1="Hello"
string2="World"

# Compare strings using [[ ]]
if [[ "$string1" == "$string2" ]]; then
    echo "Strings are equal"
else
    echo "Strings are not equal"
fi

```

How will you debug problems encountered in the shell program?

Some standard methods of debugging the problems in the script are:

use of set-x to enable debugging
Insert debug statements in a shell script to display information that helps in the identification of the problem. 

```
#!/bin/bash

# Enable debug mode
set -x

# Define variables
a=10
b=20

# Perform arithmetic operations
sum=$((a + b))
product=$((a * b))

# Print the results
echo "Sum: $sum"
echo "Product: $product"
_________________________

+ a=10
+ b=20
+ sum=30
+ product=200
+ echo 'Sum: 30'
Sum: 30
+ echo 'Product: 200'
Product: 200

```
How can you print a substring in bash?

In Bash, you can print a substring of a string using parameter expansion with the ${parameter:offset:length} syntax.
parameter is the variable containing the string.
offset is the starting position of the substring (0-based index).
length is the length of the substring to print. If length is omitted, the substring extends to the end of the original string.
By adjusting the offset and length parameters, you can print different substrings from the original string.

 Here's how you can do it:

```
#!/bin/bash

# Define a string
my_string="Hello, World!"

# Print a substring starting from a specific offset
echo "${my_string:7}"   # Output: World!

# Print a substring starting from a specific offset with a specified length
echo "${my_string:7:5}" # Output: World

```

7. Please tell us how you will check if a file exists on the filesystem.

Using the following:
```
if [ -f /var/log/messages ]

then

echo "File exists."

fi
```
9. What is the difference between [[ $string == "efg*" ]] and [[ $string == efg* ]] ?
[[ $string == efg* ]] – checks if string begins with efg. 

[[ $string == "efg*" ]] – checks if string is efg. 

10. In what ways, shell script get input values?
a. By reading command: read -p "Destination backup Server: " dest host

b. By parameters: ./script param1 param2

4) What is the equivalent of a file shortcut that we have a window on a Linux system?

Shortcuts are created using “links” on Linux. There are two types of links that can be used namely “soft link” and “hard link”.

5) What is the difference between soft and hard links?

Soft links are link to the file name and can reside on different filesytem as well; however hard links are link to the inode of the file and have to be on the same filesytem as that of the file. Deleting the original file makes the soft link inactive (broken link) but does not affect the hard link (Hard link will still access a copy of the file)

18) What is the difference between $$ and $!?

\$$ gives the process id of the currently executing process whereas $! Shows the process id of the process that recently went into the background.

What are zombie processes?

These are the processes which have died but whose exit status is still not picked by the parent process. These processes even if not functional still have its process id entry in the process table.

21) I want to monitor a continuously updating log file, what command can be used to most efficiently achieve this?

We can use tail –f filename. This will cause only the default last 10 lines to be displayed on std o/p which continuously shows the updating part of the file.

22) I want to connect to a remote server and execute some commands, how can I achieve this?

We can use ssh to do this:

ssh username@serverIP -p sshport

Example

ssh root@122.52.251.171 -p 22

Once above command is executed, you will be asked to enter the password

23) I have 2 files and I want to print the records which are common to both.

We can use “comm” command as follows:

comm -12 file1 file2 … 12 will suppress the content which are

unique to 1st and 2nd file respectively.

24) Write a script to print the first 10 elements of Fibonacci series.

```
#!/bin/bash

# Definir la cantidad de términos en la serie Fibonacci
n=10

# Inicializar los primeros dos términos de la serie
a=0
b=1

# Mostrar los primeros dos términos de la serie
echo "Serie Fibonacci de $n términos:"
echo -n "$a, $b"

# Calcular y mostrar los siguientes términos de la serie
for ((i=2; i<n; i++))
do
    # Calcular el siguiente término
    c=$((a + b))
    
    # Mostrar el término calculado
    echo -n ", $c"
    
    # Actualizar los valores de a y b para el próximo cálculo
    a=$b
    b=$c
done

echo ""  # Salto de línea al final

```
How will you connect to a database server from Linux?
We can use isql utility that comes with open client driver as follows:

isql –S serverName –U username –P password

26) What are the 3 standard streams in Linux?

0 – Standard Input1 – Standard Output2 – Standard Error

27) I want to read all input to the command from file1 direct all output to file2 and error to file 3, how can I achieve this?

command <file1 1>file2 2>file3

30) Given a file find the count of lines containing the word “ABC”.

grep –c “ABC” file1

31) What is the difference between grep and egrep?

egrep is Extended grep that supports added grep features like “+” (1 or more occurrence of a previous character),”?”(0 or 1 occurrence of a previous character) and “|” (alternate matching)

34) How will you find the total disk space used by a specific user?

du -s /home/user1 ….where user1 is the user for whom the total disk space needs to be found.

32) How to set an array in Linux?

Syntax in ksh:

Set –A arrayname= (element1 element2 ….. element)
In bash
A=(element1 element2 element3 …. elementn)
33) Write down the syntax of “for ” loop

Syntax:
```
for  iterator in (elements)
do
execute commands
done
```

34) How will you find the total disk space used by a specific user?

du -s /home/user1 ….where user1 is the user for whom the total disk space needs to be found.

35) Write the syntax for “if” conditionals in Linux?
Syntax


If  condition is successful
then
execute commands
else
execute commands
fi
36) What is the significance of $?

The command $? gives the exit status of the last command that was executed.

How do we delete all blank lines in a file?
sed  '^ [(backslash)011(backslash)040]*$/d' file1
where (backslash)011 is an octal equivalent of space and

(backslash)040 is an octal equivalent of the tab

How will I insert a line “ABCDEF” at every 100th line of a file?
sed ‘100i\ABCDEF’ file1

39) Write a command sequence to find all the files modified in less than 2 days and print the record count of each.

find . –mtime -2 –exec wc –l {} \;

How can we find the process name from its process id?

We can use "ps –p ProcessId"

42) What are the four fundamental components of every file system on Linux?

Bootblock, super block, inode block and Datablock are found fundamental components of every file system on Linux.

43) What is a boot block?

This block contains a small program called “Master Boot record”(MBR) which loads the kernel during system boot up.

44) What is a super block?

Super block contains all the information about the file system like the size of file system, block size used by its number of free data blocks and list of free inodes and data blocks.

45 What is an inode block?

This block contains the inode for every file of the file system along with all the file attributes except its name.

46) How can I send a mail with a compressed file as an attachment?

zip file1.zip file1|mailx –s “subject” Recipients email id

Email content

EOF

47) How do we create command aliases in a shell?

alias Aliasname=”Command whose alias is to be created”.

49 AWK

awk is a powerful text-processing tool available on Unix and Linux command lines. It's primarily used to search and manipulate text in text files or the output of other commands.
La sintaxis básica de awk es la siguiente:

bash
Copy code
awk 'patrón {acción}' archivo
Donde:

'patrón' es una expresión que define qué líneas procesar.
'{acción}' es el conjunto de comandos que se ejecutarán en las líneas que coincidan con el patrón.
'archivo' es el archivo de texto que awk va a procesar. Si no se especifica ningún archivo, awk procesará la entrada estándar.
Aquí tienes algunos ejemplos de cómo puedes usar awk:

Imprimir una Columna Específica de un Archivo CSV:

bash
Copy code
awk -F ',' '{print $1}' archivo.csv
Esto imprimirá la primera columna del archivo CSV archivo.csv, asumiendo que el delimitador es una coma.

Buscar y Filtrar Líneas que Contienen una Palabra Específica:

bash
Copy code
awk '/patrón/' archivo
Esto imprimirá todas las líneas del archivo que contienen la palabra 'patrón'.

Calcular la Suma de una Columna Numérica:

bash
Copy code
awk '{suma += $1} END {print suma}' archivo
Esto calculará la suma de la primera columna numérica del archivo y la imprimirá al final del procesamiento.

Contar el Número de Líneas en un Archivo:

bash
Copy code
awk 'END {print NR}' archivo
Esto imprimirá el número total de líneas en el archivo.

Estos son solo algunos ejemplos básicos de cómo puedes utilizar awk. Es una herramienta muy versátil y potente que puedes utilizar para realizar una amplia variedad de tareas de procesamiento de texto en la línea de comandos.


50 SED

sed stands for "stream editor" and it's a powerful command-line tool used for performing text transformations on an input stream or files. It's particularly useful for tasks such as search and replace, text substitution, and line-by-line processing.

Here are some key features and common uses of sed:

Search and Replace:

You can use sed to search for a pattern in a text file or input stream and replace it with another pattern. For example:

bash
Copy code
sed 's/pattern/replacement/g' file.txt
This command searches for all occurrences of "pattern" in file.txt and replaces them with "replacement".

Delete Lines:

sed can delete lines from a file or input stream based on a pattern. For example:

bash
Copy code
sed '/pattern/d' file.txt
This command deletes all lines from file.txt that contain the pattern "pattern".

Insert, Append, and Change Text:

You can use sed to insert, append, or change text in specific lines of a file. For example:

bash
Copy code
sed '2i\Inserting text on line 2' file.txt
This command inserts the text "Inserting text on line 2" before line 2 in file.txt.

Regular Expressions:

sed supports powerful regular expressions for pattern matching and substitution. This allows you to perform complex text transformations. For example:

bash
Copy code
sed 's/[0-9]\+/#&/g' file.txt
This command replaces all occurrences of numbers with "#number" in file.txt.

In-Place Editing:

sed can edit files in-place, meaning it modifies the original file rather than writing to standard output. For example:

bash
Copy code
sed -i 's/pattern/replacement/g' file.txt
This command replaces all occurrences of "pattern" with "replacement" in file.txt and saves the changes to the file.

These are just a few examples of what you can do with sed. It's a versatile and powerful tool for text manipulation on the command line.

# Kubernetes

- api server: entrypoint to K8s cluster, expose Kubernetes API

- controller manager: keeps track of whats happening in the cluster (node controller, replication controller, endpoint controller, service account and token controllers)

- Scheduler: ensures pods placement

- etcd: kubernetes backing store, key value store for critical cluster info

- si se desplega en nube se adiciona un nuevo elemento el cloud controller manager 

NODE PROCESS

each node has multiple POD in it

3 processes must be installed on every node

Worker Node do the actual work

  - CONTAINER RUNTIME (for example docker)
  - KUBELET interact with both the container runtime and node, starts the pod with a container inside
  - KUBEPROXY forwards the request, allow network communications

  ## YAML in kubernetes

always have 4 fields
- apiVersion: version create object (v1, apps/v1)
- kind: refers type of objects (pod, replicaset, deployment, service)
- metadata: is data about the object form or dictionary (name, labels) you can specify names or lables that k8s expect
- specs: aditional information to k8s esepcification depend on object (dictionary) - is a item in the list

## Workload kubernetes

- Deployments: Deployments are a Kubernetes resource type used to manage and scale a set of identical Pods. A Deployment ensures that a specified number of Pod replicas are running at any given time, and it can perform rolling updates and rollbacks to the application.

- StatefulSets: StatefulSets are used for stateful applications that require stable, unique network identifiers and persistent storage. They are useful for applications like databases where each instance needs its own identity and storage.

- DaemonSets: DaemonSets ensure that all (or some) nodes run a copy of a Pod. They are typically used for system-level daemons, logging agents, or monitoring agents that need to run on every node.

- Jobs: Jobs manage short-lived and batch workloads that run to completion, such as data processing tasks, backups, or periodic tasks. Once a Job completes successfully, Kubernetes terminates the Pod associated with the Job.

- CronJobs: CronJobs are used to schedule Jobs to run at specific times or intervals, similar to the cron utility in Unix-like operating systems. They are useful for running periodic tasks, backups, or cleanup jobs.

- Pods: While not technically a workload in itself, Pods are the smallest deployable units in Kubernetes. A Pod encapsulates one or more containers, storage resources, and network configurations. Pods can be managed directly, but they are often managed by higher-level controllers like Deployments or StatefulSets.

## Services types

In Kubernetes, Services are an abstraction that define a logical set of Pods and a policy by which to access them. Kubernetes Services enable network communication to Pods from both inside and outside of the cluster. There are several types of Services in Kubernetes:

- ClusterIP:

- This is the default type of Service in Kubernetes.
- Exposes the Service on a cluster-internal IP.
- The Service is only reachable from within the cluster.
- This type is commonly used for inter-service communication within the cluster.

- NodePort:

- Exposes the Service on a static port on each Node's IP.
The Service is accessible from outside the cluster using the <NodeIP>:<NodePort> combination.
- Kubernetes allocates a specific port (NodePort) on each node, and traffic is forwarded to the Service.
- This type is useful when you need to expose a Service externally, but it's not recommended for production use due to security concerns and limitations.

- LoadBalancer:

- Exposes the Service externally using a cloud provider's load balancer.
- The cloud provider allocates a load balancer, and traffic is routed to the Service's Pods.
- This type is useful for exposing Services externally in production environments.
- It's important to note that this type is only available if your Kubernetes cluster is running in a supported cloud provider environment.

- ExternalName:

Maps the Service to the contents of the externalName field.
Allows access to external services by returning a CNAME record with the configured externalName.
This type is commonly used to integrate with external services that are not part of the Kubernetes cluster.

- Headless:

This is a special type of Service that does not have a cluster-internal IP.
It's used to create DNS records for individual Pods without load balancing or proxying.
When you query the DNS for the Service name, Kubernetes returns the IP addresses of the Pods directly.
This type is useful for stateful applications that require stable network identities, such as databases.

## Ingress

 Ingress provides a flexible and powerful way to expose HTTP and HTTPS routes from outside the Kubernetes cluster to services within the cluster. It enables sophisticated routing and traffic management capabilities, making it an essential component for managing external access to applications running in Kubernetes.

Ingress is the most useful if you want to expose multiple services under the same IP address, and these services all use the same L7 protocol (typically HTTP). You only pay for one load balancer if you are using the native GCP integration, and because Ingress is “smart” you can get a lot of features out of the box (like SSL, Auth, Routing, etc)

### Label and selectors

Labels are properties attach to each item, selector help to filters this items
annotation are use for other purpouse, detail, version, etc

### Taints and Tolerations

Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods.

Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.

### Cordon and Drain 

- Cordon:

- When you "cordon" a node, you mark it as unschedulable. This means that no new Pods will be scheduled onto the node.
- Existing Pods on the node will continue to run as usual.
- Cordon prevents the Kubernetes scheduler from placing new Pods onto the node, ensuring that no additional workload is assigned to a node that is about to be taken offline for maintenance.

- Drain:

- When you "drain" a node, you evict all the Pods running on the node and mark the node as unschedulable.
- Evicting Pods means gracefully terminating them and rescheduling them onto other nodes in the cluster.
- The Kubernetes scheduler ensures that evicted Pods are rescheduled onto other nodes that have sufficient resources.
- Drain also ensures that any local storage associated with the node is safely unmounted or moved to another node to prevent data loss.
- Drain is typically used when you want to decommission a node for maintenance, such as applying updates, replacing hardware, or scaling down the cluster.

In summary, "cordon" prevents new Pods from being scheduled onto a node, while "drain" safely evicts existing Pods from a node and marks it as unschedulable, allowing for maintenance or decommissioning tasks to be performed without disrupting running workloads. These commands are essential for ensuring high availability and reliability in Kubernetes clusters during node maintenance operations.

### PDB

Pod Disruption Budget (PDB) is a policy object that allows you to control the disruption caused by voluntary disruptions, such as evictions due to node maintenance or scale-down operations. PDBs ensure that a specified number of Pods belonging to a replication controller, replica set, or stateful set are available during voluntary disruptions, thus ensuring the high availability of your application.

### Node Affinity

Node affinity is conceptually similar to nodeSelector, allowing you to constrain which nodes your Pod can be scheduled on based on node labels.

### CONFIGMAP

A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.

### Sidecar and Init container

Init Containers:

- Init containers are specialized containers that run before the main application containers in a Pod.
- They are primarily used for performing initialization tasks or setup procedures that need to be completed before the main application container starts.
- Init containers run to completion before any other containers in the Pod start, and they share the same network namespace and storage volumes with the main containers.
- Common use cases for init containers include:
  - Downloading and unpacking data or configuration files.
  - Performing database schema migrations or setup tasks.
  - Initializing shared storage volumes.
- Init containers provide a way to decouple complex initialization logic from the main application container, improving maintainability and flexibility.

Sidecar Containers:

- Sidecar containers are additional containers that run alongside the main application container in a Pod.
- They are used to extend or enhance the functionality of the main application container without modifying its code or configuration.
- Sidecar containers share the same lifecycle as the main application container and are co-located on the same Pod, allowing them to communicate over localhost.
- Common use cases for sidecar containers include:
  - Logging: A sidecar container can collect logs from the main application container and forward them to a centralized logging service.
  - Monitoring: A sidecar container can collect metrics and monitor the health of the main application container.
  - Proxying or networking: A sidecar container can handle network traffic routing, load balancing, or encryption for the main application container.
- Sidecar containers provide a modular and composable architecture for extending the functionality of applications running in Kubernetes, enabling features such as logging, monitoring, security, and networking to be added independently of the main application logic.

## HPA policy

HPA policies provide fine-grained control over how Kubernetes autoscales deployments and replica sets based on resource utilization metrics and custom metrics. By configuring HPA policies, you can ensure that your applications automatically scale up or down to handle changes in demand while optimizing resource utilization and maintaining performance.

## Liveness and Readiness probe

 liveness probes and readiness probes in Kubernetes lies in their purpose and effect on the functioning of a Pod:

- Liveness Probe:

Purpose: The liveness probe determines whether the containers within a Pod are still running and healthy.

Effect: If the liveness probe fails, Kubernetes restarts the affected container.

Use Case: Liveness probes are useful for detecting and recovering from situations where the application within the container has crashed or become unresponsive. They help maintain the overall health of the application by ensuring that Kubernetes replaces failed containers automatically.

- Readiness Probe:

Purpose: The readiness probe determines whether the containers within a Pod are ready to start serving traffic.

Effect: If the readiness probe fails, Kubernetes stops sending network traffic to the affected Pod.

Use Case: Readiness probes are used to prevent sending traffic to Pods that are not yet fully initialized or are experiencing issues that could cause them to serve errors or degrade performance. They allow Kubernetes to ensure that only Pods in a fully operational state receive network traffic.

In summary, while both liveness and readiness probes help ensure the reliability and availability of applications running in Kubernetes, they serve different purposes. Liveness probes detect and recover from failures within containers, while readiness probes control when Pods are considered ready to receive network traffic. Both probes are essential for managing the lifecycle of Pods effectively and maintaining the overall health of applications in a Kubernetes cluster.

## Control plane and data plane

Control plane is responsible for managin and cotrolling the overall state of the system in kubernetes has 4 major component (contorller, api server, etc and scheduler

Data plane is responsbile for managng the traffic that flows through the system)

## POD and COntainer

Pod is the smallest deployable object in kubernetes and represents one or more container, A container is a lightweight, standalone, and executable software package that includes everything needed to run a piece of software, including the code, runtime, system libraries, and settings. 

# Linux

||||
|-|-|-|
|/|Root Directory|Is the starting point of the linux file system hierarchy|
|/bin|Binary Programs|Contains essential binary executables (programs) that are available to all users. Common commands like ls,cp,mv, etc are located here|
|/boot|Bootloaders Files|Contains boot-related files, such as the linux kernel, initial RAM disk (initrd), and boot loader configurations. The boot loader is responsible for loading the operating system during the boot process|
|/dev|Devices Files|Contains device files that represent various hardware devices on the system. These files allow access to devices such as hard drives, USB devices, serial ports,etc|
|/etc|System configuration Files|Contains system-wide configuration files and scripts. Configuration files for various services, network settings, user information,etc, are stored here|
|/home|Home directories|Contains personal home directories for each user on the system. Each user has a subdirectory here with their username, where they can store their personal files and settings|
|/lib|Shared libraries|Contains shared libraries requiered by the system and various programs. These libraries provide common functions and features to multiple applications|
|/mnt|Mount directory|Used as a temporary mount point for mounting external file systems, such as removable media or networks shares|
|/opt|Optional Software|Typically used for installing additional software packages that are not part of the default system installation. Third party applications or programs that are not managed by the system's package manager may be installed here|
|/proc|Process Information|A virtual file system that provides information about running processes and system resources. Each process has a directory named after its process ID (PID), containing information about the proccess|
|/sbin|System binaries|Contains system-related binaries (programs) that are mostly used by system administrator. Commands necessary for systems maintenance or system recovery are located here|
|/tmp|Temporary files|Used for storing temporary files created by the system and users. Files in this directory are typically deleted when the system is rebooted|
|/usr|User Programs Data| Contains user-related programs, libraries, and data files It is one of the largest directories and is further divided into subdirectories such as /usr/bin, usr/lib, usr/include, etc|
|/var|Variable Data|Contains variable files that change during system|


## Linux package manager
On Linux, the package manager will change based on the Linux distribution you choose.

|Distribution(s)|	Package manager|
|-|-|
|Ubuntu, Debian|	apt-get|
|Red Hat, CentOS|	yum|
|OpenSUSE|	zypper|
|Fedora|	dnf|

## Some Commands

- whatis command:breve descripcion de un comando

- du estimate file space usage
- df report file system disk space usage
- more -n \<filename> this option display the text upto the specified n number of lines of the document
- more +n \<filename> displays the text from the specified n number of lines of the document
- head -n \<filename> primeras 10 lineas del docuemnto por defecto
- tail -n \<filename> ultimas lineas del documento por defecto 10 (si no se pone n)
- sed -n 'initial_line, last_linep' \<filename>

 (sed -n '6,12p' xyz.txt) 
 
  imprimir un rango de lineas de un documento
- grep is a filter command

grep[options] "string/pattern" file/files

## STDIN, STDOUT, STERR
A file descriptor is simply an integer number to identify STDIN, STDOUT and STDERR

0: STDIN
1: STDOUT
2: STDERR

## permisions

![](![alt text](image-88.png))
![](![alt text](image-89.png))

```
* R - read    -> 4
* W - Write   -> 2
* X - excute  -> 1



# HELM

# Networking

What is the network?

According to Merriam-Webster, Network is usually an informally interconnected group or association of different entities like a person, computers, radio stations, etc.

For example, Dominos has a network of 1232 branches across India. As the name suggests the computer network is a system of peripherals or computers interconnected with each other and has a standard communication channel established between them to exchange different types of information and data.

How are Network types classified?
Network types can be classified and divided based on the area of distribution of the network. The below diagram would help to understand the same:

![alt text](image-20.png)

Explain LAN (Local Area Network)

LANs are widely used to connect computers/laptops and consumer electronics which enables them to share resources (e.g., printers, fax machines) and exchange information. When LANs are used by companies or organizations, they are called enterprise networks. There are two different types of LAN networks i.e. wireless LAN (no wires involved achieved using Wi-Fi) and wired LAN (achieved using LAN cable). Wireless LANs are very popular these days for places where installing wire is difficult. The below diagrams explain both wireless and wired LAN.

Tell me something about VPN (Virtual Private Network)

VPN or the Virtual Private Network is a private WAN (Wide Area Network) built on the internet. It allows the creation of a secured tunnel (protected network) between different networks using the internet (public network). By using the VPN, a client can connect to the organization’s network remotely

What are the advantages of using a VPN?

Below are few advantages of using VPN:

- VPN is used to connect offices in different geographical locations remotely and is cheaper when compared to WAN connections.
- VPN is used for secure transactions and confidential data transfer between multiple offices located in different geographical locations.
- VPN keeps an organization’s information secured against any potential threats or intrusions by using virtualization.
- VPN encrypts the internet traffic and disguises the online identity.

What are the different types of VPN?
Few types of VPN are:

- Access VPN: Access VPN is used to provide connectivity to remote mobile users and telecommuters. It serves as an alternative to dial-up connections or ISDN (Integrated Services Digital Network) connections. It is a low-cost solution and provides a wide range of connectivity.
- Site-to-Site VPN: A Site-to-Site or Router-to-Router VPN is commonly used in large companies having branches in different locations to connect the network of one office to another in different locations. There are 2 sub-categories as mentioned below:
- Intranet VPN: Intranet VPN is useful for connecting remote offices in different geographical locations using shared infrastructure (internet connectivity and servers) with the same accessibility policies as a private WAN (wide area network).
- Extranet VPN: Extranet VPN uses shared infrastructure over an intranet, suppliers, customers, partners, and other entities and connects them using dedicated connections.
7. What are nodes and links?

- Node: Any communicating device in a network is called a Node. Node is the point of intersection in a network. It can send/receive data and information within a network. Examples of the node can be computers, laptops, printers, servers, modems, etc.

- Link: A link or edge refers to the connectivity between two nodes in the network. It includes the type of connectivity (wired or wireless) between the nodes and protocols used for one node to be able to communicate with the other.

 What is the network topology?

Network topology is a physical layout of the network, connecting the different nodes using the links. It depicts the connectivity between the computers, devices, cables, etc.

9. Define different types of network topology

The different types of network topology are given below:

- Bus Topology:
All the nodes are connected using the central link known as the bus.
It is useful to connect a smaller number of devices.
If the main cable gets damaged, it will damage the whole network.

- Star Topology:
All the nodes are connected to one single node known as the central node.
It is more robust.
If the central node fails the complete network is damaged.
Easy to troubleshoot.
Mainly used in home and office networks.

- Ring topology
Each node is connected to exactly two nodes forming a ring structure
If one of the nodes are damaged, it will damage the whole network
It is used very rarely as it is expensive and hard to install and manage

- Mesh Topology:
Each node is connected to one or many nodes.
It is robust as failure in one link only disconnects that node.
It is rarely used and installation and management are difficult.

- Tree Topology:
A combination of star and bus topology also know as an extended bus topology.
All the smaller star networks are connected to a single bus.
If the main bus fails, the whole network is damaged.

- Hybrid:

It is a combination of different topologies to form a new topology.
It helps to ignore the drawback of a particular topology and helps to pick the strengths from other.

10. What is an IPv4 address? What are the different classes of IPv4?

An IP address is a 32-bit dynamic address of a node in the network. An IPv4 address has 4 octets of 8-bit each with each number with a value up to 255.

IPv4 classes are differentiated based on the number of hosts it supports on the network. There are five types of IPv4 classes and are based on the first octet of IP addresses which are classified as Class A, B, C, D, or E.

|IPv4 Class|IPv4 Start|IPv4 End Address|Usage|
|-|-|-|-|	
A |	0.0.0.0	    |127.255.255.255	|Used for Large Network
B |	128.0.0.0	|191.255.255.255	|Used for Medium Size Network
C |  192.0.0.0	|223.255.255.255	|Used for Local Area Network
D |	224.0.0.0	|239.255.255.255	|Reserved for Multicasting
E |	240.0.0.0	|255.255.255.254	|Study and R&D

11. What are Private and Special IP addresses?

Private Address: For each class, there are specific IPs that are reserved specifically for private use only. This IP address cannot be used for devices on the Internet as they are non-routable.

|IPv4 Class|	Private IPv4 Start Address|	Private IPv4 End Address|
|-|-|-|	
A|	10.0.0.0	|10.255.255.255
B|	172.16.0.0	|172.31.255.255
C|	192.168.0.0	|192.168.255.255

Special Address: IP Range from 127.0.0.1 to 127.255.255.255 are network testing addresses also known as loopback addresses are the special IP address.

Describe the OSI Reference Model

|Layer|	Unit Exchanged	|Description|
|-|-|-|	
Physical|	Bit	|- It is concerned with transmitting raw bits over a communication channel. - Chooses which type of transmission mode is to be selected for the transmission. The available transmission modes are Simplex, Half Duplex and Full Duplex.,
Data Link	|Frame|The main task of this layer is to transform a raw transmission facility into a line that appears free of undetected transmission errors. It also allows detecting damaged packets using the CRC (Cyclic Redundancy Check) error-detecting, code. When more than one node is connected to a shared link, Data Link Layer protocols are required to determine which device has control over the link at a given time. It is implemented by protocols like CSMA/CD, CSMA/CA, ALOHA, and Token Passing.
Network	|Packet	|It controls the operation of the subnet. The network layer takes care of feedback messaging through ICMP messages.
Transport|	TPDU - Transaction Protocol Data Unit|The basic functionality of this layer is to accept data from the above layers, split it up into smaller units if needed, pass these to the network layer, and ensure that all the pieces arrive correctly at the other end. The Transport Layer takes care of Segmentation and Reassembly.
Session	|SPDU - Session Protocol Data Unit	| The session layer allows users on different machines to establish sessions between them. Dialogue control is using the full-duplex link as half-duplex. It sends out dummy packets from the client to the server when the client is ideal.
Presentation|	PPDU - Presentation Protocol Data Unit	|The presentation layer is concerned with the syntax and semantics of the information transmitted.It translates a message from a common form to the encoded format which will be understood by the receiver.
Application	|APDU - Application Protocol Data Unit|	It contains a variety of protocols that are commonly needed by users.The application layer sends data of any size to the transport layer.

![alt text](image-21.png)

3. Describe the TCP/IP Reference Model
It is a compressed version of the OSI model with only 4 layers. It was developed by the US Department of Defence (DoD) in the 1980s. The name of this model is based on 2 standard protocols used i.e. TCP (Transmission Control Protocol) and IP (Internet Protocol).

4. Define the 4 different layers of the TCP/IP Reference Model

![alt text](image-25.png)


Layers of TCP/IP
Layer	|Description|
|-|-|
Link|	Decides which links such as serial lines or classic Ethernet must be used to meet the needs of the connectionless internet layer.
Internet| The internet layer is the most important layer which holds the whole architecture together. It delivers the IP packets where they are supposed to be delivered.
Transport|	Its functionality is almost the same as the OSI transport layer. It enables peer entities on the network to carry on a conversation.
Application|	It contains all the higher-level protocols.

OSI Vs TCP/IP

OSI Reference Model	|TCP/IP Reference Model
|-|-|
7 layered architecture|	4 layered architecture
Fixed boundaries and functionality for each layer|	Flexible architecture with no strict boundaries between layers
Low Reliability	| High Reliability
Vertical Layer Approach|	Horizontal Layer Approach

### HTTP and the HTTPS protocol?

HTTP is the HyperText Transfer Protocol which defines the set of rules and standards on how the information can be transmitted on the World Wide Web (WWW).  It helps the web browsers and web servers for communication. It is a ‘stateless protocol’ where each command is independent with respect to the previous command. HTTP is an application layer protocol built upon the TCP. It uses port 80 by default.

HTTPS is the HyperText Transfer Protocol Secure or Secure HTTP. It is an advanced and secured version of HTTP. On top of HTTP, SSL/TLS protocol is used to provide security. It enables secure transactions by encrypting the communication and also helps identify network servers securely. It uses port 443 by default.

### GET, POST, PUT, PATCH, DELTE

- GET The GET method is used to retrieve data from the server.
- POST The POST method sends data to the server and creates a new resource.
- PUT The PUT method is most often used to update an existing resource.
- PAtCH the request is very similar to the PUT request, but the body of the request contains only the property of the resource that needs to be changed. The response is the new version of the resource.
- DELETE The DELETE method is used to delete a resource specified by its URI.

### URI

URI (Uniform Resource Identifier) es una cadena de caracteres que identifica un recurso específico en la web. Es una secuencia de caracteres que proporciona un identificador único y global para un recurso, ya sea una página web, un archivo, un servicio, un fragmento de texto o cualquier otro recurso que pueda ser identificado.

La URI consta de dos partes principales:

URL (Uniform Resource Locator):

Una URL es un tipo de URI que especifica la ubicación de un recurso en la web, incluyendo el protocolo de acceso (como HTTP o HTTPS), el nombre del dominio o la dirección IP del servidor, y la ruta al recurso en el servidor. Por ejemplo, https://www.ejemplo.com/pagina.html es una URL que apunta a una página web llamada "pagina.html" en el servidor "www.ejemplo.com" utilizando el protocolo HTTPS.
URN (Uniform Resource Name):

Un URN es otro tipo de URI que proporciona un nombre único y persistente para un recurso, independientemente de su ubicación o acceso. A diferencia de una URL, que especifica dónde se puede encontrar un recurso, un URN simplemente proporciona un identificador único para el recurso. Ejemplos de URN incluyen ISBNs para libros y DOIs para documentos científicos.
En resumen, una URI es una cadena de caracteres que proporciona una identificación única y global para un recurso en la web, ya sea especificando su ubicación (URL) o proporcionando un nombre único (URN).

###  SMTP protocol

SMTP is the Simple Mail Transfer Protocol. SMTP sets the rule for communication between servers. This set of rules helps the software to transmit emails over the internet. It supports both End-to-End and Store-and-Forward methods. It is in always-listening mode on port 25.


### DNS

DNS is the Domain Name System. It is considered as the devices/services directory of the Internet. It is a decentralized and hierarchical naming system for devices/services connected to the Internet. It translates the domain names to their corresponding IPs. For e.g. interviewbit.com to 172.217.166.36. It uses port 53 by default.

9. What is the use of a router and how is it different from a gateway?

The router is a networking device used for connecting two or more network segments. It directs the traffic in the network. It transfers information and data like web pages, emails, images, videos, etc. from source to destination in the form of packets. It operates at the network layer. The gateways are also used to route and regulate the network traffic but, they can also send data between two dissimilar networks while a router can only send data to similar networks.

###  TCP protocol

TCP or TCP/IP is the Transmission Control Protocol/Internet Protocol. It is a set of rules that decides how a computer connects to the Internet and how to transmit the data over the network. It creates a virtual network when more than one computer is connected to the network and uses the three ways handshake model to establish the connection which makes it more reliable.

### UDP protocol

UDP is the User Datagram Protocol and is based on Datagrams. Mainly, it is used for multicasting and broadcasting. Its functionality is almost the same as TCP/IP Protocol except for the three ways of handshaking and error checking. It uses a simple transmission without any hand-shaking which makes it less reliable.

### Compare between TCP and UDP

TCP/IP	|UDP
|-|-|
Connection-Oriented Protocol	|Connectionless Protocol
More Reliable	| Less Reliable
Slower Transmission	|Faster Transmission
Packets order can be preserved or can be rearranged	|Packets order is not fixed and packets are independent of each other
Uses three ways handshake model for connection|	No handshake for establishing the connection
TCP packets are heavy-weight|	UDP packets are light-weight
Offers error checking mechanism |	No error checking mechanism
Protocols like HTTP, FTP, Telnet, SMTP, HTTPS, etc use TCP at the transport layer |	Protocols like DNS, RIP, SNMP, RTP, BOOTP, TFTP, NIP, etc use UDP at the transport layer

![alt text](image-22.png)

## ICMP protocol

ICMP is the Internet Control Message Protocol. It is a network layer protocol used for error handling. It is mainly used by network devices like routers for diagnosing the network connection issues and crucial for error reporting and testing if the data is reaching the preferred destination in time. It uses port 7 by default.

## DHCP Protocol

DHCP is the Dynamic Host Configuration Protocol.

It is an application layer protocol used to auto-configure devices on IP networks enabling them to use the TCP and UDP-based protocols. The DHCP servers auto-assign the IPs and other network configurations to the devices individually which enables them to communicate over the IP network. It helps to get the subnet mask, IP address and helps to resolve the DNS. It uses port 67 by default.

##  ARP protocol

ARP is Address Resolution Protocol. It is a network-level protocol used to convert the logical address i.e. IP address to the device's physical address i.e. MAC address. It can also be used to get the MAC address of devices when they are trying to communicate over the local network.

## FTP protocol

FTP is a File Transfer Protocol. It is an application layer protocol used to transfer files and data reliably and efficiently between hosts. It can also be used to download files from remote servers to your computer. It uses port 27 by default.

What is the MAC address and how is it related to NIC

MAC address is the Media Access Control address. It is a 48-bit or 64-bit unique identifier of devices in the network. It is also called the physical address embedded with Network Interface Card (NIC) used at the Data Link Layer. NIC is a hardware component in the networking device using which a device can connect to the network.

9Differentiate the MAC address with the IP address

The difference between MAC address and IP address are as follows:

MAC Address	|IP Address
|-|-|
Media Access Control Address|	Internet Protocol Address
6 or 8-byte hexadecimal number|	4 (IPv4) or 16 (IPv6) Byte address
It is embedded with NIC	| It is obtained from the network
Physical Address|	Logical Address
Operates at Data Link Layer|	Operates at Network Layer.
Helps to identify the device|	Helps to identify the device connectivity on the network.

## What is a subnet

A subnet is a network inside a network achieved by the process called subnetting which helps divide a network into subnets. It is used for getting a higher routing efficiency and enhances the security of the network. It reduces the time to extract the host address from the routing table.

## Compare the hub vs switch
Hub	|Switch
|-|-|
Operates at Physical Layer|	Operates at Data Link Layer
Half-Duplex transmission mode|	Full-Duplex transmission mode
Ethernet devices can be connectedsend|	LAN devices can be connected
Less complex, less intelligent, and cheaper	|Intelligent and effective
No software support for the administration|	Administration software support is present
Less speed up to 100 MBPS	|Supports high speed in GBPS
Less efficient as there is no way to avoid collisions when more than one nodes sends the packets at the same time|	More efficient as the collisions can be avoided or reduced as compared to Hub

What is the difference between the ipconfig and the ifconfig?

ipconfig	|ifconfig
|-|-|
Internet Protocol Configuration	|Interface Configuration
Command used in Microsoft operating systems to view and configure network interfaces	|Command used in MAC, Linux, UNIX operating systems to view and configure network interfaces
Used to get the TCP/IP summary and allows to changes the DHCP and DNS settings

##  firewall

The firewall is a network security system that is used to monitor the incoming and outgoing traffic and blocks the same based on the firewall security policies. It acts as a wall between the internet (public network) and the networking devices (a private network). It is either a hardware device, software program, or a combination of both. It adds a layer of security to the network.

## Unicasting, Anycasting, Multicasting and Broadcasting?

- Unicasting: If the message is sent to a single node from the source then it is known as unicasting. This is commonly used in networks to establish a new connection.
- Anycasting: If the message is sent to any of the nodes from the source then it is known as anycasting. It is mainly used to get the content from any of the servers in the Content Delivery System.
- Multicasting: If the message is sent to a subset of nodes from the source then it is known as multicasting. Used to send the same data to multiple receivers. 
- Broadcasting: If the message is sent to all the nodes in a network from a source then it is known as broadcasting. DHCP and ARP in the local network use broadcasting.

15. What happens when you enter google.com in the web browser?

Below are the steps that are being followed:

- Check the browser cache first if the content is fresh and present in cache display the same.
- If not, the browser checks if the IP of the URL is present in the cache (browser and OS) if not then request the OS to do a DNS lookup using UDP to get the corresponding IP address of the URL from the DNS server to establish a new TCP connection.
- A new TCP connection is set between the browser and the server using three-way handshaking.
- An HTTP request is sent to the server using the TCP connection.
- The web servers running on the Servers handle the incoming HTTP request and send the HTTP response.
- The browser process the HTTP response sent by the server and may close the TCP connection or reuse the same for future requests.
- If the response data is cacheable then browsers cache the same.
- Browser decodes the response and renders the content.


# AWs Services

What is AWS CodeCommit?

AWS CodeCommit is a fully managed source control service that hosts Git repositories, allowing teams to securely store and manage their code assets.

What is AWS CodeBuild?

AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces deployable artifacts, providing continuous integration and automated build capabilities.

What is AWS CodeDeploy?

AWS CodeDeploy is a fully managed deployment service that automates application deployments to various compute services, including Amazon EC2 instances, AWS Lambda, and on-premises servers.

How does AWS CodeDeploy work?

AWS CodeDeploy works by deploying applications from source code or artifacts stored in repositories, and then automating the deployment process across multiple instances, ensuring reliability and minimizing downtime.

What is AWS Code Pipeline?

AWS Code Pipeline is a fully managed continuous delivery service that orchestrates the build, test, and deployment of applications, enabling fast and reliable software releases.

What is AWS Elastic Beanstalk?

AWS Elastic Beanstalk is a fully managed platform-as-a-service (PaaS) that allows you to deploy and run applications without managing the underlying infrastructure. It supports various programming languages and application stacks.

What is AWS CloudFormation?

AWS CloudFormation is a service that allows you to create and manage AWS resources using declarative JSON or YAML templates, enabling infrastructure-as-code and automated provisioning.

What is AWS CloudWatch?

AWS CloudWatch is a monitoring and observability service that provides metrics, logs, and events for AWS resources and applications, allowing you to gain insights and take automated actions.

How can you automate infrastructure deployments in AWS?

You can automate infrastructure deployments in AWS by using services like AWS CloudFormation, AWS CDK (Cloud Development Kit), or Infrastructure-as-Code (IaC) tools like Terraform, enabling you to define and provision resources in a repeatable and automated manner.

What is AWS Lambda?

AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. It scales automatically and charges you based on the actual compute time consumed.

How can you automate serverless application deployments in AWS?

You can automate serverless application deployments in AWS by using services like AWS SAM (Serverless Application Model), AWS CloudFormation, or CI/CD tools integrated with AWS Lambda, allowing you to define and deploy your serverless application resources.

What is AWS X-Ray?

AWS X-Ray is a service that helps you analyze and debug distributed applications, providing insights into application performance and the ability to trace requests across various components.

What is AWS Elastic Beanstalk?

AWS Elastic Beanstalk is a fully managed platform-as-a-service (PaaS) that allows you to deploy and run applications without managing the underlying infrastructure. It supports various programming languages and application stacks.

What is AWS OpsWorks?

AWS OpsWorks is a configuration management service that uses Chef or Puppet to automate the provisioning and management of applications and resources in AWS.

What is AWS Systems Manager?

AWS Systems Manager provides a unified interface for managing and monitoring resources in AWS, allowing you to automate operational tasks, configure resources, and collect insights.

What is AWS CloudTrail?

AWS CloudTrail is a service that enables governance, compliance, and auditing of AWS account activities by recording API calls and storing the resulting logs for analysis.

What is AWS Elastic Load Balancer (ELB)?

AWS Elastic Load Balancer automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, enhancing the availability and scalability of applications.

What is AWS Auto Scaling?

AWS Auto Scaling automatically adjusts the capacity of AWS resources, such as EC2 instances or DynamoDB throughput, based on predefined scaling policies, ensuring optimal performance and cost efficiency.

What is the difference between horizontal and vertical scaling?

Horizontal scaling involves adding more instances or resources to distribute the load across multiple machines, while vertical scaling involves increasing the resources of an existing instance or machine to handle increased load.

What is the purpose of AWS CloudWatch Events?

AWS CloudWatch Events allows you to respond to operational changes in your AWS resources by triggering automated actions based on events, such as changes to EC2 instances or S3 buckets.

What is AWS Systems Manager Parameter Store?

AWS Systems Manager Parameter Store is a managed service that allows you to store and retrieve configuration data and secrets, such as database credentials, as secure key-value pairs.

What is AWS Secrets Manager?

AWS Secrets Manager is a secrets management service that helps you protect access to applications, services, and resources by securely storing and managing secrets like API keys and database credentials.

How can you ensure security in AWS environments?

You can ensure security in AWS environments by following security best practices, implementing proper identity and access management (IAM), encrypting data at rest and in transit, implementing network security controls, and regularly auditing and monitoring your resources.

What is AWS Identity and Access Management (IAM)?

AWS IAM is a web service that enables you to securely control access to AWS resources. It allows you to create and manage users, groups, roles, and permissions to grant appropriate access to resources.

What is AWS Key Management Service (KMS)?

AWS Key Management Service (KMS) is a managed service that allows you to create and control the encryption keys used to encrypt your data stored in AWS services or applications.

What is the AWS Well-Architected Framework?

The AWS Well-Architected Framework provides guidance on designing and operating secure, high-performing, resilient, and efficient infrastructure in the cloud.

How can you implement high availability in AWS?

You can implement high availability in AWS by using services like Elastic Load Balancer, Auto Scaling, and deploying resources across multiple Availability Zones (AZs) to ensure redundancy and fault tolerance.

What is AWS CloudFormation Change Sets?

AWS CloudFormation Change Sets allow you to preview and deploy the changes to your AWS CloudFormation stacks before making any actual modifications, reducing the risk of unintended changes.

What is the AWS Server Migration Service?

The AWS Server Migration Service is a service that helps migrate on-premises workloads to AWS, enabling seamless and automated migration of virtual machines, physical servers, and databases.

What is AWS Artifact?

AWS Artifact is a portal that provides access to AWS compliance reports, such as Service Organization Control (SOC) reports, Payment Card Industry (PCI) reports, and other security and compliance documentation.

What is the AWS Well-Architected Tool?

The AWS Well-Architected Tool is a service that helps review and improve the architectures of workloads running on AWS, providing recommendations based on the best practices outlined in the AWS Well-Architected Framework.

How can you implement fault tolerance in AWS?

You can implement fault tolerance in AWS by designing applications to automatically recover from failures using features like Auto Scaling, Elastic Load Balancer, and multiple Availability Zones.

What is AWS CloudFront?

AWS CloudFront is a global content delivery network (CDN) service that accelerates the delivery of your web content, including dynamic, static, and streaming content.

What is AWS Route 53?

AWS Route 53 is a scalable and highly available domain name system (DNS) web service that routes end users to applications by translating domain names into IP addresses.

What is AWS S3?

AWS S3 (Simple Storage Service) is a highly scalable object storage service that allows you to store and retrieve data from anywhere on the web. It provides durability, availability, and security for your data.

What is AWS RDS?

AWS RDS (Relational Database Service) is a managed database service that simplifies the setup, operation, and scaling of relational databases, such as MySQL, PostgreSQL, Oracle, and SQL Server.

What is AWS DynamoDB?

AWS DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to handle large amounts of data and high-traffic applications.

What is AWS CloudWatch Logs?

AWS CloudWatch Logs is a service that allows you to monitor, store, and access log files from AWS resources and applications. It helps you troubleshoot issues, track system behavior, and gain insights from log data.

What is AWS CloudTrail?

AWS CloudTrail is a service that enables governance, compliance, and auditing of AWS account activities by recording API calls and storing the resulting logs for analysis.

What is AWS Kinesis?

AWS Kinesis is a platform for streaming data at any scale, enabling you to collect, process, and analyze real-time streaming data from various sources.

What is AWS SQS?

AWS SQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale distributed systems by allowing components to communicate asynchronously.

What is AWS SNS?

AWS SNS (Simple Notification Service) is a fully managed messaging service that enables you to send notifications and messages to individuals or groups using various communication protocols.

What is AWS Elasticache?

AWS Elasticache is a fully managed in-memory caching service that helps improve the performance and scalability of web applications by caching frequently accessed data.

What is AWS Step Functions?

AWS Step Functions is a serverless workflow service that enables you to coordinate multiple AWS services and build applications using visual workflows.

What is AWS Batch?

AWS Batch is a fully managed service that enables you to run batch computing workloads at any scale, efficiently managing and optimizing the distribution of your workloads across EC2 instances.

What is AWS Cloud9?

AWS Cloud9 is a cloud-based integrated development environment (IDE) that allows you to write, run, and debug code in a browser. It supports various programming languages and integrates with other AWS services.

What is the AWS Command Line Interface (CLI)?

The AWS Command Line Interface (CLI) is a unified tool that allows you to manage and control your AWS services from the command line. It provides a simple and efficient way to interact with AWS resources.

What is AWS Systems Manager Session Manager?

AWS Systems Manager Session Manager is a fully managed service that provides secure and auditable instance management without the need for SSH or RDP access. It allows you to establish secure shell (SSH) or remote desktop protocol (RDP) connections to your instances directly from the AWS Management Console.

What is AWS CloudFormation StackSets?

AWS CloudFormation StackSets allow you to create, update, or delete stacks across multiple AWS accounts and regions, simplifying the management of resources and configurations at scale.

What is the AWS Serverless Application Model (SAM)?

The AWS Serverless Application Model (SAM) is an open-source framework that extends AWS CloudFormation to simplify the deployment and management of serverless applications on AWS.

What is AWS CodeStar?

AWS CodeStar is a fully managed service that provides a unified user interface, integrations, and templates for developing, building, and deploying applications on AWS.

What is AWS CodeArtifact?

AWS CodeArtifact is a fully managed artifact repository service that allows you to securely store, publish, and share software packages, dependencies, and artifacts.

What is AWS CodeGuru?

AWS CodeGuru is a developer tool powered by machine learning that provides automated code reviews and performance recommendations, helping you improve the quality and efficiency of your applications.

What is AWS CodeDeploy?

AWS CodeDeploy is a fully managed deployment service that automates application deployments to various compute services, including Amazon EC2 instances, AWS Lambda, and on-premises servers.

What is AWS CodeCommit?

AWS CodeCommit is a fully managed source control service that hosts Git repositories, allowing teams to securely store and manage their code assets.

What is AWS CodeBuild?

AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces deployable artifacts, providing continuous integration and automated build capabilities.

What is AWS CodePipeline?

AWS CodePipeline is a fully managed continuous delivery service that orchestrates the build, test, and deployment of applications, enabling fast and reliable software releases.

What is AWS CodeStar Connections?

AWS CodeStar Connections is a service that allows you to securely connect your third-party Git repositories, such as GitHub or Bitbucket, to AWS development tools like AWS CodePipeline and AWS CodeBuild.

What is AWS CodeStar Notifications?

AWS CodeStar Notifications is a service that provides a unified interface to configure and manage notifications for events occurring in your software development lifecycle (SDLC) tools, such as code commits, build completions, and deployment status changes.

What is AWS AppConfig?

AWS AppConfig is a managed application configuration service that helps you deploy and manage application configurations across distributed systems, allowing you to quickly roll out changes and ensure configuration consistency.

What is AWS Cloud9?

AWS Cloud9 is a cloud-based integrated development environment (IDE) that allows you to write, run, and debug code in a browser. It supports various programming languages and integrates with other AWS services.

What is AWS CloudFormation StackSets?

AWS CloudFormation StackSets allow you to create, update, or delete stacks across multiple AWS accounts and regions, simplifying the management of resources and configurations at scale.

What is AWS Systems Manager Session Manager?

AWS Systems Manager Session Manager is a fully managed service that provides secure and auditable instance management without the need for SSH or RDP access. It allows you to establish secure shell (SSH) or remote desktop protocol (RDP) connections to your instances directly from the AWS Management Console.

What is AWS Secrets Manager?

AWS Secrets Manager is a secrets management service that helps you protect access to applications, services, and resources by securely storing and managing secrets like API keys and database credentials.

What is AWS Step Functions?

AWS Step Functions is a serverless workflow service that enables you to coordinate multiple AWS services and build applications using visual workflows.

What is AWS Lambda?

AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. It scales automatically and charges you based on the actual compute time consumed.

What is AWS Elastic Beanstalk?

AWS Elastic Beanstalk is a fully managed platform-as-a-service (PaaS) that allows you to deploy and run applications without managing the underlying infrastructure. It supports various programming languages and application stacks.

What is AWS Elastic Load Balancer (ELB)?

AWS Elastic Load Balancer automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, enhancing the availability and scalability of applications.

What is AWS Auto Scaling?

AWS Auto Scaling automatically adjusts the capacity of AWS resources, such as EC2 instances or DynamoDB throughput, based on predefined scaling policies, ensuring optimal performance and cost efficiency.

What is AWS CloudWatch?

AWS CloudWatch is a monitoring and observability service that provides metrics, logs, and events for AWS resources and applications, allowing you to gain insights and take automated actions.

What is AWS X-Ray?

AWS X-Ray is a service that helps you analyze and debug distributed applications, providing insights into application performance and the ability to trace requests across various components.

What is AWS CloudTrail?

AWS CloudTrail is a service that enables governance, compliance, and auditing of AWS account activities by recording API calls and storing the resulting logs for analysis.

What is AWS CodeDeploy?

AWS CodeDeploy is a fully managed deployment service that automates application deployments to various compute services, including Amazon EC2 instances, AWS Lambda, and on-premises servers.

What is AWS CodeCommit?

AWS CodeCommit is a fully managed source control service that hosts Git repositories, allowing teams to securely store and manage their code assets.

What is AWS CodeBuild?

AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces deployable artifacts, providing continuous integration and automated build capabilities.

What is AWS CodePipeline?

AWS CodePipeline is a fully managed continuous delivery service that orchestrates the build, test, and deployment of applications, enabling fast and reliable software releases.

What is AWS OpsWorks?

AWS OpsWorks is a configuration management service that uses Chef or Puppet to automate the provisioning and management of applications and resources in AWS.

What is AWS Systems Manager?

AWS Systems Manager provides a unified interface for managing and monitoring resources in AWS, allowing you to automate operational tasks, configure resources, and collect insights.

What is AWS CloudFormation?

AWS CloudFormation is a service that allows you to create and manage AWS resources using declarative JSON or YAML templates, enabling infrastructure-as-code and automated provisioning.

What is AWS CloudFront?

AWS CloudFront is a global content delivery network (CDN) service that accelerates the delivery of your web content, including dynamic, static, and streaming content.

What is AWS Route 53?

AWS Route 53 is a scalable and highly available domain name system (DNS) web service that routes end users to applications by translating domain names into IP addresses.

_____________________

## AWS arquitect

Being well-prepared is crucial for any tech interview, especially when aiming for a specialized position in cloud computing. For those eager to secure a role in cloud architecture, understanding the nuances of AWS-related questions can significantly bolster your chances.

In this article, we discuss AWS interview questions that may come up when interviewing for Solution Architect roles. For these cloud jobs the AWS Certified Solutions Architect — Associate certification is a highly valuable certification to earn.

The AWS Certified Solutions Architect — Associate is a fundamental certification geared towards those who design distributed systems and applications on the AWS platform. It’s an invaluable credential for individuals stepping into the world of AWS architecture or seeking to deepen their existing knowledge.

The role of an AWS Solutions Architect Associate is pivotal in the modern cloud ecosystem. With an ever-growing demand for cloud services, businesses are keen on hiring professionals who can design, manage, and implement sophisticated cloud solutions on AWS. This means, as a candidate, one should be well-prepared to address both fundamental and intricate aspects of AWS services and best practices.

The AWS Solutions Architect Associate certification is evidence of your advanced knowledge in this area, but to excel in an interview, you need more than just the certification. You need an understanding of real-world scenarios, challenges, and solutions.

Whether you are preparing for an upcoming interview or are on the hiring side and need to gauge a candidate’s depth of knowledge, this list of 20 comprehensive interview questions will set you up for success.

Let’s dive into these AWS architect interview questions and help you gear up for success in your interview.

1. How would you design a fault-tolerant architecture on AWS?

Answer: Designing a fault-tolerant architecture in AWS involves utilizing multiple Availability Zones for redundancy, implementing Elastic Load Balancing to distribute incoming traffic across instances, auto-scaling to match demand, and using AWS services like Amazon S3 and Amazon RDS for data durability. Regularly backing up data and having a disaster recovery plan in place, along with monitoring system health using Amazon CloudWatch, are also critical practices.

2. What are the benefits of using Amazon EC2 instances within an Auto Scaling group?

Answer: Auto Scaling ensures that Amazon EC2 instances adjust according to the defined conditions, maintaining application availability and balancing capacity. It helps in cost reduction by adjusting the number of instances in use based on demand, thereby avoiding the need to pay for idle computing resources. Auto Scaling in various instances across multiple Availability Zones can also increase the fault tolerance of your applications.

3. Explain the significance of a Virtual Private Cloud (VPC) in AWS.

Answer: A VPC enables you to launch AWS resources into a virtual network that you’ve defined. This virtual network closely resembles a traditional network that you’d operate in your own data center, with the benefits of using the scalable infrastructure of AWS. It provides control over your virtual networking environment, including selection of your own IP address range, the creation of subnets, and configuration of route tables and network gateways.

4. What strategies would you use to optimize the costs of AWS services for a project?

Answer: Cost optimization in AWS can involve several strategies: choosing the right pricing models (e.g., Reserved Instances, Spot Instances), correctly estimating traffic and choosing the appropriate instance types, using Auto Scaling to adjust resources, monitoring and analyzing with AWS Cost Explorer, utilizing cheaper storage options for infrequently accessed data, and employing AWS Budgets and AWS Trusted Advisor for cost monitoring and recommendations.

5. How can AWS Direct Connect be beneficial for an organization?

Answer: AWS Direct Connect allows an organization to establish a dedicated network connection between one’s network and AWS data centers. This provides a more stable and reliable connection and can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections. It’s particularly beneficial for high throughput workloads or transferring large amounts of data.


Click the image above to watch this video from our youtube channel

6. In a hybrid cloud architecture, how can you securely integrate on-premises datacenters with AWS?

Answer: Secure integration in a hybrid cloud model can be achieved through several means: AWS VPN allows you to establish a secure and private encrypted tunnel from your network or device to the AWS global network. AWS Direct Connect bypasses the public Internet and establishes a secure, dedicated connection from your premises to AWS. Additionally, using AWS Transit Gateway, you can connect your on-premises datacenters to AWS with a single gateway, simplifying your network and putting in place more stringent security measures.

7. What is Amazon S3’s consistency model?

Answer: Amazon S3 provides strong read-after-write consistency for PUTS of new objects and eventual consistency for overwrite PUTS and DELETES. This means that if a new object is written to S3, any subsequent retrieval requests will return the latest version of the object. However, for updates and deletes, it might take some time for the changes to propagate, and requests made in the interim might return old data.

8. How do you ensure high availability and disaster recovery in AWS?

Answer: High availability and disaster recovery involve multiple AWS services and features:

Utilize multiple Availability Zones and Regions to ensure that applications can handle the loss of entire data centers.
Implement Amazon RDS or Amazon Aurora Multi-AZ deployments to automate database setup, patching, and backups.
Use Amazon S3 for durable, scalable, and secure object storage with built-in lifecycle policies for automated backup and storage management.
Employ AWS CloudFormation for infrastructure as code and quick re-provisioning of resources in a disaster recovery scenario.
Implement AWS Shield and AWS WAF for resilience against DDoS attacks.
9. How does AWS assist in the deployment of hybrid applications?

Answer: AWS offers various services to facilitate hybrid deployments. AWS Outposts extends AWS’s infrastructure, services, APIs, and tools to virtually any datacenter or on-premises facility for a truly consistent hybrid experience. AWS Storage Gateway connects on-premises software applications with cloud-based storage. Amazon RDS on VMware lets you deploy managed databases in on-premises VMware environments, and AWS Direct Connect establishes a dedicated network connection from an on-premises network to AWS.

10. What are the key aspects to consider while planning a migration to AWS cloud?

Answer: Key considerations include:

Assessing the existing on-premises infrastructure and understanding the technical requirements.
Deciding on a suitable migration strategy (like re-hosting, re-platforming, re-factoring, re-purchasing, retiring, or retaining).
Calculating the total cost of ownership and potential cost savings.
Planning for security and compliance.
11: How do Amazon S3 transfer acceleration and Amazon CloudFront differ in terms of content delivery?

Answer: Amazon S3 Transfer Acceleration is specifically designed to speed up transferring files to and from Amazon S3 by utilizing Amazon CloudFront’s globally distributed edge locations. When users upload or download files, the data will travel through the optimized network path to reach the S3 bucket faster. On the other hand, Amazon CloudFront is a content delivery network (CDN) that caches content in edge locations around the world, bringing the content closer to the end-users and reducing latency. While both involve CloudFront’s edge locations, S3 Transfer Acceleration is for faster transfers to S3, and CloudFront is for general content distribution to end-users.

12. What are placement groups in EC2, and can you describe the different types?

Answer: Placement groups are a way of controlling how EC2 instances are physically located relative to one another. There are three types:

Cluster Placement Groups: Used for applications needing low network latency and high network throughput, ensuring instances are placed in a single availability zone.

Spread Placement Groups: Ensures that instances are placed on distinct underlying hardware, reducing correlated failures and suitable for a small number of critical instances.

Partition Placement Groups: Spread instances across different partitions, ensuring that instances in one partition do not share the underlying hardware with instances in other partitions.

13. Describe AWS Organizations and its primary use cases. How does it help in managing multiple AWS accounts?

Answer: AWS Organizations lets you consolidate multiple AWS accounts into an organization that you create and centrally manage. Primary use cases include centralized billing, setting up and managing accounts, applying and managing service control policies across accounts, and creating a hierarchical, multi-account structure. AWS Organizations simplifies billing for multiple accounts by enabling the setup of a single payment method for all the accounts in your organization through consolidated billing.

14. How would you design a multi-region architecture for high availability on AWS?

Answer: Designing a multi-region architecture involves replicating data and applications in more than one geographic region. This is achieved by setting up application stacks in multiple AWS regions, utilizing Amazon Route 53 for geo-based routing, replicating data using services like Amazon RDS cross-region replication or S3 Cross-Region Replication, and ensuring stateless applications to quickly scale and replicate.

15. What is the difference between an Application Load Balancer (ALB) and a Network Load Balancer (NLB)? When would you choose one over the other?

Answer: ALB is layer 7 (application layer) load balancer, suitable for routing user traffic based on content type, path, or host in the request. It’s ideal for HTTP/HTTPS traffic. NLB operates at layer 4 (transport layer) and is designed for TCP/UDP traffic where extreme performance is required. NLB is chosen for ultra-high levels of traffic or when low-level routing is necessary.

16. Explain the process of automating infrastructure deployment using AWS CloudFormation. What are CloudFormation templates?

Answer: AWS CloudFormation automates and simplifies the task of repeatedly and predictably creating groups of related resources that power your applications. The process involves writing a CloudFormation template in JSON or YAML format. This template defines the AWS resources you want to deploy. Once the template is created, you can use CloudFormation to create a stack based on the template, which will provision the defined resources.

17. Describe the benefits of using Amazon Aurora over traditional RDS databases. How does Aurora ensure fault tolerance and scalability?

Answer: Amazon Aurora is a MySQL and PostgreSQL-compatible relational database that combines the speed and availability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases. Benefits include up to 5 times the performance of MySQL and 3 times the performance of PostgreSQL. Aurora automatically divides your database volume into 10GB segments spread across many disks. Each 10GB chunk of your database volume is replicated six ways, across three Availability Zones. Aurora continuously backs up your data to Amazon S3, and transparently recovers from physical storage failures; instance failover typically takes less than 30 seconds.

18. How can AWS WAF be integrated with AWS services to enhance web application security?

Answer: AWS WAF (Web Application Firewall) protects web applications from common web exploits. It can be integrated with Amazon CloudFront (the CDN service) and Application Load Balancer, allowing you to create custom rules that block malicious traffic patterns. This means that you can use AWS WAF to protect both your applications accessed via CloudFront distributions and those accessed directly via an Application Load Balancer.

19. What’s the difference between AWS Systems Manager and AWS OpsWorks? How do they help in configuration management?

Answer: AWS Systems Manager provides a unified interface for viewing operational data from multiple AWS services and allows you to automate operational tasks across AWS resources. It aids in patch management, automation, config management, and instance management. On the other hand, AWS OpsWorks is a configuration management service that uses Chef and provides instances of Chef and Puppet. OpsWorks lets you model and set up your Amazon EC2 instances and other AWS resources with Chef cookbooks or Puppet manifests. Both tools assist in automating infrastructure and application management tasks but differ in their approaches and integration points.

20. Explain the purpose and use cases of Amazon Kinesis. How does it compare to traditional messaging systems like SQS or SNS?

Answer: Amazon Kinesis is a platform to stream data on AWS, offering powerful services to make it easier to load and analyze streaming data. Use cases include real-time analytics, dashboards, and telemetry. While SQS (Simple Queue Service) is a distributed message queuing service and SNS (Simple Notification Service) is for pub/sub messaging, Kinesis provides real-time data streaming. SQS and SNS are ideal for decoupling components and sending notifications, while Kinesis focuses on real-time data processing.

# CLOUDFORMATION

1. What is AWS CloudFormation, and how does it fit into a DevOps environment?

Answer: AWS CloudFormation is a service that allows you to provision and manage AWS infrastructure as code. In a DevOps environment, CloudFormation enables the automated and repeatable creation of AWS resources, making it an essential tool for infrastructure deployment and management.

2. Explain the concept of an AWS CloudFormation stack and its components.

Answer: An AWS CloudFormation stack is a collection of AWS resources defined in a CloudFormation template. Stack components include the template, the stack name, and the set of resources created and managed by that stack.

3. What are AWS CloudFormation templates, and in what formats can they be written?

Answer: AWS CloudFormation templates are JSON or YAML text files that define the AWS resources and their configurations. You can write templates in either JSON or YAML format. YAML is often preferred for its human-readable and concise syntax.

4. How does CloudFormation support the concept of Infrastructure as Code (IaC), and what are the advantages of using IaC for infrastructure management?

Answer: CloudFormation supports IaC by allowing infrastructure to be defined and managed in code. IaC provides advantages such as version control, repeatability, documentation, and automated provisioning, resulting in more reliable and maintainable infrastructure.

5. What are CloudFormation stack policies, and how can they be used to control updates to a CloudFormation stack?

Answer: CloudFormation stack policies are JSON documents that control what can be updated in a stack. They allow you to specify which resources are immutable, ensuring that specific resources are not modified during stack updates.

6. Explain the concept of intrinsic functions in CloudFormation templates and provide examples of when to use functions like `Fn::Ref` or `Fn::Sub`.

Answer: Intrinsic functions are used for dynamic value generation in CloudFormation templates. For example:
— `Fn::Ref` is used to reference other resources.
— `Fn::Sub` is used for variable substitution.
These functions are helpful for creating templates with dynamic behavior.

7. How can you handle sensitive data such as passwords or API keys in CloudFormation templates securely?

Answer: Sensitive data should be handled using AWS Secrets Manager or AWS Systems Manager Parameter Store. You can reference these secure stores in CloudFormation templates, ensuring that sensitive information is not exposed in the template itself.

8. What is the difference between a CloudFormation change set and a stack update? When would you use each?

Answer: A change set is a preview of the changes to a stack that will occur during an update. It allows you to review and understand the changes before they are applied. You would use a change set when you want to assess the impact of an update without actually making changes to the stack.

9. How do you handle dependencies between AWS resources in a CloudFormation template, and why is it important to define resource dependencies?

Answer: You handle dependencies using the `DependsOn` attribute in CloudFormation templates to specify the order in which resources are created or updated. Defining dependencies is essential for ensuring that resources are created or updated in the correct sequence, preventing issues related to resource availability.

10. Explain how you can extend CloudFormation templates using AWS Cloud Development Kit (CDK) or custom resources. When would you choose one method over the other?

Answer: You can extend CloudFormation templates using the AWS CDK to define templates programmatically in a programming language. Alternatively, you can use custom resources to extend templates with custom logic. The choice depends on your familiarity with the technologies and the complexity of the extensions required. CDK is often preferred for its ease of use and integrated development experience.

11. What is AWS CloudFormation drift detection, and how does it help maintain infrastructure integrity over time?

Answer: AWS CloudFormation drift detection allows you to detect and identify differences between the desired stack configuration defined in the template and the actual deployed stack. It helps ensure that the infrastructure remains in the desired state by identifying any configuration changes or drift.

12. Explain the process of creating and managing a stack in AWS CloudFormation. Provide an example command to create a stack.

Answer: You can create and manage a stack using the `aws cloudformation` commands. An example command to create a stack:

aws cloudformation create-stack — stack-name MyStack — template-body file://my-template.yaml

## CloudFormation helper scripts reference

AWS CloudFormation provides the following Python helper scripts that you can use to install software and start services on an Amazon EC2 instance that you create as part of your stack:

cfn-init: Use to retrieve and interpret resource metadata, install packages, create files, and start services.

cfn-signal: Use to signal with a CreationPolicy or WaitCondition, so you can synchronize other resources in the stack when the prerequisite resource or application is ready.

cfn-get-metadata: Use to retrieve metadata for a resource or path to a specific key.

cfn-hup: Use to check for updates to metadata and execute custom hooks when changes are detected.
